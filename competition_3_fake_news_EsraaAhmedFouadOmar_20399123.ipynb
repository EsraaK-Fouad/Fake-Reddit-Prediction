{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPPsnZWUottE"
      },
      "source": [
        "-----------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYJvQV7EottG"
      },
      "source": [
        "# **✔️ Problem Formulation**\n",
        "\n",
        "- ## Define the problem\n",
        "\n",
        "   - The important part before starting to do any thing, we understand the problem very well to be able to be soundness about features you need and features have effect on your prediction.\n",
        "   \n",
        "   - Our goal of this problem is to predict if a specific reddit post is fake news or not, by looking at its title. As we knew that False information on the Internet has caused many social problems due to the raise of social network and its role in different. So, it is important to design model helps us in classifying our news. \n",
        "   \n",
        "  - Our output is in categories from 0 to 1. This is a binary classification task. Given a data sample (contains various forms of words), we are going to predict the probability (0-1, float).\n",
        "   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQZP5imCottH"
      },
      "source": [
        "# Liberaries \n",
        "-----------------------------------------------\n",
        "----------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqMyPEtrGeED",
        "outputId": "6ecdbc31-b7f7-436b-b4c7-3b98a453fb2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.9/dist-packages (1.7.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from xgboost) (1.22.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from xgboost) (1.10.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-optimize in /usr/local/lib/python3.9/dist-packages (0.9.0)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.9/dist-packages (from scikit-optimize) (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.9/dist-packages (from scikit-optimize) (1.22.4)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.9/dist-packages (from scikit-optimize) (1.10.1)\n",
            "Requirement already satisfied: pyaml>=16.9 in /usr/local/lib/python3.9/dist-packages (from scikit-optimize) (21.10.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.9/dist-packages (from scikit-optimize) (1.1.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.9/dist-packages (from pyaml>=16.9->scikit-optimize) (6.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.20.0->scikit-optimize) (3.1.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from nltk) (4.65.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk) (1.1.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/dist-packages (from nltk) (2022.10.31)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.9/dist-packages (1.0.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from langdetect) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install xgboost\n",
        "!pip install scikit-optimize\n",
        "!pip install nltk\n",
        "!pip install langdetect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-03-29T01:33:05.891042Z",
          "iopub.status.busy": "2023-03-29T01:33:05.890627Z",
          "iopub.status.idle": "2023-03-29T01:33:17.426125Z",
          "shell.execute_reply": "2023-03-29T01:33:17.424513Z",
          "shell.execute_reply.started": "2023-03-29T01:33:05.891007Z"
        },
        "id": "CHPs6LuDottJ",
        "outputId": "3849f719-0ac6-438d-b3cb-6afe4ab6ce9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: deep_translator in /usr/local/lib/python3.9/dist-packages (1.10.1)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.9/dist-packages (from deep_translator) (4.11.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.9/dist-packages (from deep_translator) (2.27.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep_translator) (2.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (2.0.12)\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import sklearn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk \n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost.sklearn import XGBClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from xgboost import XGBClassifier\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
        "from langdetect import detect\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "!pip install deep_translator\n",
        "from deep_translator import GoogleTranslator\n",
        "from sklearn.preprocessing import MinMaxScaler ,OneHotEncoder, StandardScaler , LabelEncoder\n",
        "from imblearn.over_sampling import SMOTE \n",
        "from imblearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "import joblib\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from sklearn.model_selection import PredefinedSplit\n",
        "from skopt import BayesSearchCV\n",
        "import scipy.stats as ss\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn import metrics\n",
        "import seaborn as sns\n",
        "import math\n",
        "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (20,15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOT_aYHSGeET",
        "outputId": "76e1ebca-16ed-41a1-ea1d-66d591666d9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Package abc is already up-to-date!\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Package alpino is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    |   Package bcp47 is already up-to-date!\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Package chat80 is already up-to-date!\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package city_database is already up-to-date!\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    |   Package comtrans is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Package crubadan is already up-to-date!\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Package dolch is already up-to-date!\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Package floresta is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Package ieer is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Package indian is already up-to-date!\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    |   Package jeita is already up-to-date!\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Package kimmo is already up-to-date!\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    |   Package knbc is already up-to-date!\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    |   Package machado is already up-to-date!\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Package paradigms is already up-to-date!\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Package pe08 is already up-to-date!\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Package pil is already up-to-date!\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Package pl196x is already up-to-date!\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Package porter_test is already up-to-date!\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Package ppattach is already up-to-date!\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    |   Package propbank is already up-to-date!\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Package ptb is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Package qc is already up-to-date!\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    |   Package reuters is already up-to-date!\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Package rslp is already up-to-date!\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Package rte is already up-to-date!\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    |   Package semcor is already up-to-date!\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Package senseval is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Package smultron is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Package state_union is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Package swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Package switchboard is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Package tagsets is already up-to-date!\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Package timit is already up-to-date!\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Package toolbox is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Package webtext is already up-to-date!\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2022 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Package ycoe is already up-to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "nltk.download('all')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-29T00:41:08.097595Z",
          "iopub.status.busy": "2023-03-29T00:41:08.096528Z",
          "iopub.status.idle": "2023-03-29T00:41:08.103606Z",
          "shell.execute_reply": "2023-03-29T00:41:08.102636Z",
          "shell.execute_reply.started": "2023-03-29T00:41:08.097535Z"
        },
        "id": "eZNEL76nottK"
      },
      "outputs": [],
      "source": [
        "pd.set_option(\"display.max.columns\", None)#that’s probably more readable than wrapping long rows\n",
        "pd.set_option(\"display.max.rows\", None)#that’s probably more readable than wrapping long columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljvxclcqottK"
      },
      "source": [
        "# Read Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-03-29T00:41:08.105267Z",
          "iopub.status.busy": "2023-03-29T00:41:08.104931Z",
          "iopub.status.idle": "2023-03-29T00:41:08.342275Z",
          "shell.execute_reply": "2023-03-29T00:41:08.340900Z",
          "shell.execute_reply.started": "2023-03-29T00:41:08.105236Z"
        },
        "id": "WdeSUAVUottL",
        "outputId": "0ba50641-21da-4855-d795-11f39462d5d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of train data :  (60000, 3)\n",
            "Shape of test data :  (59151, 2)\n"
          ]
        }
      ],
      "source": [
        "train_data = pd.read_csv(\"/content/xy_train.csv\")\n",
        "test_data = pd.read_csv(\"/content/x_test.csv\")\n",
        "data = train_data.copy()\n",
        "submission_data = test_data.copy()\n",
        "print(\"Shape of train data : \",train_data.shape )\n",
        "print(\"Shape of test data : \",test_data.shape )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TOm1WiP6nfW",
        "outputId": "d3f305b9-8e48-440b-efeb-1dd91632faf3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       id                                               text  label\n",
              "0  265723  A group of friends began to volunteer at a hom...      0\n",
              "1  284269  British Prime Minister @Theresa_May on Nerve A...      0\n",
              "2  207715  In 1961, Goodyear released a kit that allows P...      0\n",
              "3  551106  Happy Birthday, Bob Barker! The Price Is Right...      0\n",
              "4    8584  Obama to Nation: 聙\"Innocent Cops and Unarmed Y...      0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-46e0997e-ff18-4440-8c11-1a5f9975c74a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>265723</td>\n",
              "      <td>A group of friends began to volunteer at a hom...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>284269</td>\n",
              "      <td>British Prime Minister @Theresa_May on Nerve A...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>207715</td>\n",
              "      <td>In 1961, Goodyear released a kit that allows P...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>551106</td>\n",
              "      <td>Happy Birthday, Bob Barker! The Price Is Right...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>8584</td>\n",
              "      <td>Obama to Nation: 聙\"Innocent Cops and Unarmed Y...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-46e0997e-ff18-4440-8c11-1a5f9975c74a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-46e0997e-ff18-4440-8c11-1a5f9975c74a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-46e0997e-ff18-4440-8c11-1a5f9975c74a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIFNimTDInVC"
      },
      "source": [
        "**Observation** 💡💡💡💡💡\n",
        "\n",
        "- As you can see, we have a training set (with labels), and a testing set (without labels). The training set  consists of one feature. In other hand, testing set has the same feature except the target column.\n",
        "- Size of training data is  60000 which is pretty good 😇😇😇. Machine learning likes large data. The data is the backbone of machine learning\n",
        "\n",
        "- We noticed that the data is not clean 😳😳😳. As you can see, there some special characters. the data have to clean to get a good model. Garbage in garbage out 🔥🔥🔥🔥. We will clean it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rrwh6lhYottP"
      },
      "source": [
        "# Helper Functions \n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-03-29T00:42:17.353576Z",
          "iopub.status.busy": "2023-03-29T00:42:17.353170Z",
          "iopub.status.idle": "2023-03-29T00:42:17.394183Z",
          "shell.execute_reply": "2023-03-29T00:42:17.393154Z",
          "shell.execute_reply.started": "2023-03-29T00:42:17.353537Z"
        },
        "id": "StheKdbUottP",
        "outputId": "46acfab1-b6bb-4881-a530-65a85850f3c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "class preprocessing:\n",
        "    \n",
        "    \n",
        "    def __init__(self, dataframe):\n",
        "        self.dataframe = dataframe\n",
        "        \n",
        "        \n",
        "    #------------------------------- Check null ^_^ ------------------------------------\n",
        "    '''\n",
        "    This function counts null in each column in the dataframe and calculate the percent of nulls in the column then return the \n",
        "    dataframe consist of 2 columns :  one contains count of null values in each column and second contains percent \n",
        "    '''\n",
        "    def null_values(self):\n",
        "        null_val = pd.DataFrame(self.dataframe.isnull().sum())\n",
        "        null_val.columns = ['null_val']\n",
        "        null_val['percent_'] = round(null_val['null_val'] / len(self.dataframe.index), 2) * 100\n",
        "        null_val = null_val.sort_values('percent_', ascending = False)\n",
        "        return null_val\n",
        "    #------------------------------- Check duplication ^_^  ------------------------------------\n",
        "    '''\n",
        "    This function counts duplicated rows in the dataframe \n",
        "    '''\n",
        "    def duplicated_values(self):\n",
        "        return print(\"Number of duplicated rows\" , self.dataframe.duplicated().sum())\n",
        "    #--------------------------------------- drop columns  ^_^ ---------------------------------------   \n",
        "    '''\n",
        "    This function drop duplicated columns \n",
        "    '''\n",
        "    \n",
        "    def drop_col(self):\n",
        "        self.dataframe = self.dataframe.drop_duplicates()\n",
        "        return self.dataframe\n",
        "  \n",
        "    #-------------------------------------------------- preprocess string ^_^ -----------------------------------\n",
        "    '''\n",
        "    This function helps us to apply some preprocessing steps like lowering words, removing regex characters , removing some words aren't important ,removing numbers ,and tokenizing sentence into words \n",
        "    in the text column . This function gives three options :\n",
        "     1. use lemmati\n",
        "     \n",
        "    '''\n",
        "\n",
        "    def clean_text(self , text, for_embedding=False , for_stemming = False):\n",
        "        \"\"\" steps:\n",
        "            -  all lowercase\n",
        "            - remove any html tags (< /br> often found)\n",
        "            - Keep only ASCII + European Chars and whitespace, no digits\n",
        "            - remove single letter chars\n",
        "            - convert all whitespaces (tabs etc.) to single wspace\n",
        "            - remove numbers\n",
        "            - remove url\n",
        "            if not for embedding (but e.g. tdf-idf) and not stemming:\n",
        "            - all lowercase\n",
        "            - remove stopwords, punctuation and lemmatize\n",
        "            if for stemming :\n",
        "            - all lowercase\n",
        "            - remove stopwords, punctuation and stem\n",
        "\n",
        "        \"\"\"\n",
        "        stemmer = SnowballStemmer(\"english\")\n",
        "        stop_words = set(stopwords.words(\"english\"))\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        # text = self.detect_and_translate(text) # we tried to translate forgien words but it takes large time. So, we comment this line to save our time.\n",
        "        text=str(text)\n",
        "        text = text.lower() # lower words \n",
        "\n",
        "        RE_WSPACE = re.compile(r\"\\s+\", re.IGNORECASE)\n",
        "        RE_TAGS = re.compile(r\"<[^>]+>\")\n",
        "        RE_ASCII = re.compile(r\"[^A-Za-zÀ-ž ]\", re.IGNORECASE)\n",
        "        RE_SINGLECHAR = re.compile(r\"\\b[A-Za-zÀ-ž]\\b\", re.IGNORECASE)\n",
        "        \n",
        "        if for_embedding:\n",
        "            # Keep punctuation\n",
        "            RE_ASCII = re.compile(r\"[^A-Za-zÀ-ž,.!? ]\", re.IGNORECASE)\n",
        "            RE_SINGLECHAR = re.compile(r\"\\b[A-Za-zÀ-ž,.!?]\\b\", re.IGNORECASE)\n",
        "\n",
        "        text = re.sub(RE_TAGS, \" \", text)\n",
        "        text = re.sub(RE_ASCII, \" \", text)\n",
        "        text = re.sub(RE_SINGLECHAR, \" \", text)\n",
        "        text = re.sub(RE_WSPACE, \" \", text)\n",
        "        rem_url=re.sub(r'http\\S+', '',text) # Remove any url \n",
        "        rem_num = re.sub('[0-9]+', '', rem_url) # Remove numbers\n",
        "        tokenizer = RegexpTokenizer(r'\\w+') #\n",
        "        word_tokens = tokenizer.tokenize(rem_num)  #Tokenize sentences into words \n",
        "\n",
        "\n",
        "        if for_embedding:\n",
        "            # no stemming, lowering and punctuation / stop words removal\n",
        "            words_filtered = word_tokens\n",
        "        elif for_stemming:\n",
        "            words_filtered = [\n",
        "                  stemmer.stem(word) for word in word_tokens if word not in stop_words\n",
        "              ]\n",
        "        else:\n",
        "            words_filtered = [\n",
        "                lemmatizer.lemmatize(word) for word in word_tokens if word not in stop_words\n",
        "            ]\n",
        "\n",
        "        text_clean = \" \".join(words_filtered)\n",
        "        return text_clean\n",
        "\n",
        "      #--------------------------------------- translation  ^_^ ---------------------------------------   \n",
        "    '''\n",
        "    we discovered that there is some of forgein words in different langnague. \n",
        "    So, we implemented this function to detect and translate these words from any language into english.\n",
        "    '''\n",
        "\n",
        "    def detect_and_translate(self , text):\n",
        "        \n",
        "        result_lang = detect(text)\n",
        "        \n",
        "        if result_lang == 'en':\n",
        "            return text \n",
        "        \n",
        "        else:\n",
        "            translator = GoogleTranslator(source='auto', target='en')\n",
        "            translate_text = translator.translate(text)\n",
        "            return translate_text \n",
        "\n",
        "\n",
        "  \n",
        "           \n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')        \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rf0XHphlottM"
      },
      "source": [
        "# Explore Data \n",
        "---------------\n",
        "---------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cL0dr7eLottM"
      },
      "source": [
        "\n",
        "### Describe data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-03-29T00:41:08.403414Z",
          "iopub.status.busy": "2023-03-29T00:41:08.402938Z",
          "iopub.status.idle": "2023-03-29T00:41:08.439176Z",
          "shell.execute_reply": "2023-03-29T00:41:08.437994Z",
          "shell.execute_reply.started": "2023-03-29T00:41:08.403364Z"
        },
        "id": "PVup2J5FottM",
        "outputId": "87d42582-108f-477d-cdfc-cdf63cd2429b",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 60000 entries, 0 to 59999\n",
            "Data columns (total 3 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   id      60000 non-null  int64 \n",
            " 1   text    60000 non-null  object\n",
            " 2   label   60000 non-null  int64 \n",
            "dtypes: int64(2), object(1)\n",
            "memory usage: 1.4+ MB\n"
          ]
        }
      ],
      "source": [
        "train_data.info(verbose = True, null_counts = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b72r467SO4cb"
      },
      "source": [
        "**Observation**  💡💡💡💡💡\n",
        "- As you can see, we have a bout 3 columns. One of them is object and the rest is int64. \n",
        "- text and label columns are the most important columns. id column is useless. we will not use it. \n",
        "- There is no null values in our data 🥳🥳🥳🥳. Let's move on to discover more about this.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fLbGvRpottM"
      },
      "source": [
        "### Describe numerical features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xv9Rd6cHFIaA"
      },
      "outputs": [],
      "source": [
        "#--------------------------- Calculate length of text ^_^ ----------------------------------------\n",
        "''' \n",
        "This function calculate length of text in each row in our dataframe  \n",
        "'''\n",
        "def text_length(df_):\n",
        "  return len(df_)\n",
        "\n",
        "# apply the previous function to get column of length calculate text length in each row  \n",
        "train_data['length']  = train_data['text'].apply(text_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-03-29T00:41:08.441564Z",
          "iopub.status.busy": "2023-03-29T00:41:08.441085Z",
          "iopub.status.idle": "2023-03-29T00:41:09.472927Z",
          "shell.execute_reply": "2023-03-29T00:41:09.471712Z",
          "shell.execute_reply.started": "2023-03-29T00:41:08.441516Z"
        },
        "id": "w4uCyD5qottM",
        "outputId": "cea522ea-eed2-48b7-b532-55206eef6953"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of numerical columns in the data is :  3\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>label</th>\n",
              "      <th>length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>60000.000000</td>\n",
              "      <td>60000.000000</td>\n",
              "      <td>60000.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>280243.619717</td>\n",
              "      <td>0.467667</td>\n",
              "      <td>124.944350</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>162623.600284</td>\n",
              "      <td>0.506648</td>\n",
              "      <td>226.356902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>85.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>138541.250000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>93.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>279910.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>106.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>421243.250000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>132.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>562695.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>19668.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  id         label        length\n",
              "count   60000.000000  60000.000000  60000.000000\n",
              "mean   280243.619717      0.467667    124.944350\n",
              "std    162623.600284      0.506648    226.356902\n",
              "min         0.000000      0.000000     85.000000\n",
              "25%    138541.250000      0.000000     93.000000\n",
              "50%    279910.500000      0.000000    106.000000\n",
              "75%    421243.250000      1.000000    132.000000\n",
              "max    562695.000000      2.000000  19668.000000"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(\"Number of numerical columns in the data is : \", train_data.describe().shape[1])\n",
        "train_data.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGfR054WottN"
      },
      "source": [
        " **Observation** 💡💡💡💡💡\n",
        "\n",
        "- As we mentioned before, we aren't intersted in id column. So, we willnot discuss or analyze it.\n",
        "- As you can see, Label column has 3 catigories form 0 to 2. From problem formulation, we knew that Our output(label) is in categories from 0 to 1. So,we will deal with this problem later.\n",
        "\n",
        "- From Length column, we can see that the size of text is from 85 upto 19668 characters. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMY4CP8vottO"
      },
      "source": [
        "### Visualize data distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-03-29T00:41:49.470090Z",
          "iopub.status.busy": "2023-03-29T00:41:49.469705Z",
          "iopub.status.idle": "2023-03-29T00:42:16.736461Z",
          "shell.execute_reply": "2023-03-29T00:42:16.735202Z",
          "shell.execute_reply.started": "2023-03-29T00:41:49.470052Z"
        },
        "id": "CnTPl7ayottO",
        "outputId": "77b86d0d-605b-4e0a-a434-631c9c87b157"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[<AxesSubplot:title={'center':'id'}>,\n",
              "        <AxesSubplot:title={'center':'label'}>],\n",
              "       [<AxesSubplot:title={'center':'length'}>, <AxesSubplot:>]],\n",
              "      dtype=object)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x576 with 4 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.rcParams['figure.figsize'] = [15, 8]\n",
        "train_data.hist()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation**💡💡💡💡💡\n",
        "\n",
        "- Graphs confirm our insight before that there is an extra category in the label column.\n",
        "- The difference between class 1 and class 0 in label column isn't significant. There is no imbalance in our target🥳🥳🥳🥳."
      ],
      "metadata": {
        "id": "tQzOluXcUUNX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7O3sE7YGottN"
      },
      "source": [
        "### Describe categorigal data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-output": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "execution": {
          "iopub.execute_input": "2023-03-29T00:41:09.475832Z",
          "iopub.status.busy": "2023-03-29T00:41:09.474962Z",
          "iopub.status.idle": "2023-03-29T00:41:09.529963Z",
          "shell.execute_reply": "2023-03-29T00:41:09.528774Z",
          "shell.execute_reply.started": "2023-03-29T00:41:09.475783Z"
        },
        "id": "-sPi_EYKottN",
        "outputId": "3356ae91-f039-4a97-ab5e-972429bedebb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of categorical columns in the data is :  1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                     text\n",
              "count                                               60000\n",
              "unique                                              59645\n",
              "top     /r/Fakehistoryporn subscribers as they attempt...\n",
              "freq                                                   14"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4c8a11ec-a73e-45c6-8667-bac8cf41a17a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>60000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>59645</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>/r/Fakehistoryporn subscribers as they attempt...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4c8a11ec-a73e-45c6-8667-bac8cf41a17a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4c8a11ec-a73e-45c6-8667-bac8cf41a17a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4c8a11ec-a73e-45c6-8667-bac8cf41a17a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "print(\"Number of categorical columns in the data is : \", train_data.describe(include=['O']).shape[1])\n",
        "train_data.describe(include=['O'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEX4hqlPottO"
      },
      "source": [
        "**Observations**💡💡💡💡💡\n",
        "\n",
        "\n",
        "We noticed that :\n",
        "\n",
        "- we have about 355 rows are duplicated. we will deal with these rows later.\n",
        "- There is no null values in the text column."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qy85zL8sEX-k"
      },
      "source": [
        "#### Convert type of text column from object to category\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-29T00:41:49.338004Z",
          "iopub.status.busy": "2023-03-29T00:41:49.337519Z",
          "iopub.status.idle": "2023-03-29T00:41:49.466042Z",
          "shell.execute_reply": "2023-03-29T00:41:49.464855Z",
          "shell.execute_reply.started": "2023-03-29T00:41:49.337953Z"
        },
        "id": "f7YFkMdaottN"
      },
      "outputs": [],
      "source": [
        "# Convert type of text column from object to category. Apply this on train data and submission data\n",
        "train_data['text'] = train_data['text'].astype(\"category\")\n",
        "submission_data['text'] =  submission_data['text'].astype(\"category\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpkGAnM0ottO"
      },
      "source": [
        "# Target Column\n",
        "--------------------------------------------------------------------------\n",
        "--------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-03-29T00:42:16.738491Z",
          "iopub.status.busy": "2023-03-29T00:42:16.738001Z",
          "iopub.status.idle": "2023-03-29T00:42:17.347986Z",
          "shell.execute_reply": "2023-03-29T00:42:17.346717Z",
          "shell.execute_reply.started": "2023-03-29T00:42:16.738438Z"
        },
        "id": "LX65qDwBottO",
        "outputId": "3802b38c-bedc-4443-a190-01820640877a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "classes in rating column [0 1 2]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<AxesSubplot:>"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.rcParams['figure.figsize'] = [10,10]\n",
        "print(\"classes in rating column\" , np.unique(data[\"label\"]))\n",
        "#plot count of each class in target column \n",
        "train_data[\"label\"].value_counts().plot(kind='bar')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEBaTh4vSb2y",
        "outputId": "073cffb9-bc95-436c-9069-5279369cab4d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    0.536200\n",
              "1    0.459933\n",
              "2    0.003867\n",
              "Name: label, dtype: float64"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data[\"label\"].value_counts(normalize=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxoGoM5gottP"
      },
      "source": [
        "**Observation**\n",
        "\n",
        "- The output label is 0 or 1 (fake or not). Class 2 DOESN'T belong to our target. Don't worry will solve this problem \n",
        "- There is no imbalance between classes. class 0 and 1 is almost close to each other. Class 0 is 53.6200 % while class 1 is 45.9933 %.\n",
        "- Thanks to allah, class 2 doesn't have large percent in our data. It is only 0.3%\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K72V3n_RottQ"
      },
      "source": [
        "# Apply on Training Data\n",
        "-------------------------------------------------\n",
        "--------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-29T00:42:17.395999Z",
          "iopub.status.busy": "2023-03-29T00:42:17.395645Z",
          "iopub.status.idle": "2023-03-29T00:42:17.403401Z",
          "shell.execute_reply": "2023-03-29T00:42:17.402317Z",
          "shell.execute_reply.started": "2023-03-29T00:42:17.395965Z"
        },
        "id": "fzc0_6ErottQ"
      },
      "outputs": [],
      "source": [
        "# Call the preprocessing class to discover train data and apply preprocessing methods\n",
        "preprocessing_steps = preprocessing(train_data.drop('id', axis =1))\n",
        "# Call the preprocessing class to apply preprocessing methods also on submission_data\n",
        "preprocessing_steps_sub = preprocessing(submission_data)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78WzMrkEottQ"
      },
      "source": [
        "# Explore Data\n",
        "------------------------------------------------------------\n",
        "-------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOMOQDhaottQ"
      },
      "source": [
        "- ### Check null values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "execution": {
          "iopub.execute_input": "2023-03-29T00:42:17.405139Z",
          "iopub.status.busy": "2023-03-29T00:42:17.404808Z",
          "iopub.status.idle": "2023-03-29T00:42:17.428229Z",
          "shell.execute_reply": "2023-03-29T00:42:17.427153Z",
          "shell.execute_reply.started": "2023-03-29T00:42:17.405105Z"
        },
        "id": "y7bH6cPHottQ",
        "outputId": "57459c87-67fe-47da-90c4-51896c490d88"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        null_val  percent_\n",
              "text           0       0.0\n",
              "label          0       0.0\n",
              "length         0       0.0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-151687c0-3c8b-4620-a2bc-6f43806e9c0c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>null_val</th>\n",
              "      <th>percent_</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>text</th>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>label</th>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>length</th>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-151687c0-3c8b-4620-a2bc-6f43806e9c0c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-151687c0-3c8b-4620-a2bc-6f43806e9c0c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-151687c0-3c8b-4620-a2bc-6f43806e9c0c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "preprocessing_steps.null_values()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dp2Qpm9iottR"
      },
      "source": [
        "**Observation**💡💡💡💡💡\n",
        "- There is no null values 🥳🥳🥳🥳"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQZpjyQFottR"
      },
      "source": [
        "- ### Check duplications"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-03-29T00:42:17.430533Z",
          "iopub.status.busy": "2023-03-29T00:42:17.429948Z",
          "iopub.status.idle": "2023-03-29T00:42:17.492373Z",
          "shell.execute_reply": "2023-03-29T00:42:17.491464Z",
          "shell.execute_reply.started": "2023-03-29T00:42:17.430481Z"
        },
        "id": "iQqsWMtbottR",
        "outputId": "c9962cdb-260c-4f1d-c9b8-2c6fe5c18322"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of duplicated rows 345\n"
          ]
        }
      ],
      "source": [
        "preprocessing_steps.duplicated_values()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdtUuAmOottR"
      },
      "source": [
        "**Observations**💡💡💡💡💡\n",
        "\n",
        "- Here , we  see that there are only 345 rows duplicated although we noticed in the previous sections that there are about 5 rows are duplicated in text column. we can justify that. Maybe the labels of these rows are different. That means they may take class2. Don't worry, we will recover this.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uveqqj_5ottS"
      },
      "source": [
        "\n",
        "# Preprocessing Stage \n",
        "------------------------------------\n",
        "---------------------------------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Drop duplicated rows"
      ],
      "metadata": {
        "id": "Bm7xeM9ctgnY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = preprocessing_steps.drop_col()\n",
        "print(preprocessing_steps.duplicated_values())\n",
        "print(\"Shape of train data after dropping duplicated rows\" , train_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykYteZQ3tgDB",
        "outputId": "3fa7abaa-b66d-44cd-f5a0-4f3fa6c8ba3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of duplicated rows 0\n",
            "None\n",
            "Shape of train data after dropping duplicated rows (59655, 7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Apply different preprocessing on text column"
      ],
      "metadata": {
        "id": "6jngFV9fuW9X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we tried to prepare our data to use them later in the model. we applied different preprocessing methods on text to use them in our trails and compare between them. This will give us indication know which one is the best."
      ],
      "metadata": {
        "id": "B5K7P2xixQYg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44piggrOKpuD"
      },
      "outputs": [],
      "source": [
        "# Apply lematization on text on both train and submission data\n",
        "train_data[\"text_clean_lemma\"] = train_data[\"text\"].map(lambda x: preprocessing_steps.clean_text(x, for_embedding=False, for_stemming =False) if isinstance(x, str) else x)\n",
        "submission_data[\"text_clean_lemma\"] = submission_data[\"text\"].map(lambda x: preprocessing_steps_sub.clean_text(x, for_embedding=False,for_stemming =False) if isinstance(x, str) else x)\n",
        "# Apply stemming on text on both train and submission data\n",
        "train_data[\"text_clean_stem\"] = train_data[\"text\"].map(lambda x: preprocessing_steps.clean_text(x, for_embedding=False, for_stemming = True) if isinstance(x, str) else x)\n",
        "submission_data[\"text_clean_stem\"] = submission_data[\"text\"].map(lambda x: preprocessing_steps_sub.clean_text(x, for_embedding=False , for_stemming = True) if isinstance(x, str) else x)\n",
        "# Apply special preprocessing to use cleaned data with embedding methods on text on both train and submission data\n",
        "train_data[\"text_clean_emb\"] = train_data[\"text\"].map(lambda x: preprocessing_steps.clean_text(x, for_embedding=True ,for_stemming =False) if isinstance(x, str) else x)\n",
        "submission_data[\"text_clean_emb\"] = submission_data[\"text\"].map(lambda x: preprocessing_steps_sub.clean_text(x, for_embedding=True, for_stemming =False) if isinstance(x, str) else x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create binary classification. replace 2 with nan values\n"
      ],
      "metadata": {
        "id": "uanPWSuMwfwg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data[\"label_clean\"] = 0\n",
        "train_data.loc[train_data[\"label\"] >= 1, \"label_clean\"] = 1\n",
        "train_data.loc[train_data[\"label\"] >= 2, \"label_clean\"] = np.NaN"
      ],
      "metadata": {
        "id": "I6u3gGxJwnKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFt5lS1Vw5fc",
        "outputId": "1515fcfd-d122-4be7-85fe-378d50dc1ac3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 59655 entries, 0 to 59999\n",
            "Data columns (total 7 columns):\n",
            " #   Column            Non-Null Count  Dtype  \n",
            "---  ------            --------------  -----  \n",
            " 0   text              59655 non-null  object \n",
            " 1   label             59655 non-null  int64  \n",
            " 2   length            59655 non-null  int64  \n",
            " 3   text_clean_lemma  59655 non-null  object \n",
            " 4   text_clean_stem   59655 non-null  object \n",
            " 5   text_clean_emb    59655 non-null  object \n",
            " 6   label_clean       59423 non-null  float64\n",
            "dtypes: float64(1), int64(2), object(4)\n",
            "memory usage: 3.6+ MB\n"
          ]
        }
      ],
      "source": [
        "train_data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OjHRs0ZPymAl",
        "outputId": "0781a9e1-e10f-4c55-a678-6d9f7741952f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "text                  0\n",
              "label                 0\n",
              "length                0\n",
              "text_clean_lemma      0\n",
              "text_clean_stem       0\n",
              "text_clean_emb        0\n",
              "label_clean         232\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "train_data.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation**💡💡💡💡💡\n",
        "\n",
        "- After replacing class 2 with nan, we got null values. As you can see, they are about 232 null values in label_clean column 😞😞😞😞\n",
        "- After cleaning text, we have to check if there are rows with empty text \" \".\n",
        "- Next step , we will drop  empty text \" \" and nan values(232) in label_clean."
      ],
      "metadata": {
        "id": "WzrOBUcUzqrY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfBh71h8x55V"
      },
      "outputs": [],
      "source": [
        "# Drop when any of text with empty \"\"\n",
        "train_data = train_data[(train_data[\"text_clean_stem\"] != \"\") | (train_data[\"text_clean_stem\"] != \"null\")]\n",
        "# drop nan values in columns\n",
        "train_data = train_data.dropna( axis=\"index\", subset=[\"label_clean\", \"text\", \"text_clean_lemma\" ,'text_clean_emb',\"text_clean_stem\"]).reset_index(drop=True)\n",
        "data_clean = train_data.copy() #just take a copy of train data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5ghaM9N2yX1",
        "outputId": "84c74786-77ca-4854-9968-9f5ef5e29756"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Int64Index([30, 38, 41, 50, 56, 877, 1403, 3656, 9046, 9339], dtype='int64')"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "train_data[(train_data[\"text_clean_stem\"] == \"\") | (train_data[\"text_clean_stem\"] == \"null\")].index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sARkOzzc1aat",
        "outputId": "1f380031-b186-42f0-b7b6-8bb8ddc46bb5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Int64Index([30, 38, 41, 50, 56, 881, 1409, 3668, 9100, 9394], dtype='int64')"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data[(train_data[\"text_clean_emb\"] == \"\") | (train_data[\"text_clean_emb\"] == \"null\")].index\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zM3vQBwUx6Hj",
        "outputId": "f76277b7-d276-40d3-97b3-acdfb0935d4b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Int64Index([30, 38, 41, 50, 56, 881, 1409, 3668, 9100, 9394], dtype='int64')"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data[(train_data[\"text_clean_lemma\"] == \"\") | (train_data[\"text_clean_lemma\"] == \"null\")].index\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbMegBMS21pl",
        "outputId": "74e5dffc-ae91-47f9-bac0-4c6de5f651d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 59423 entries, 0 to 59422\n",
            "Data columns (total 7 columns):\n",
            " #   Column            Non-Null Count  Dtype  \n",
            "---  ------            --------------  -----  \n",
            " 0   text              59423 non-null  object \n",
            " 1   label             59423 non-null  int64  \n",
            " 2   length            59423 non-null  int64  \n",
            " 3   text_clean_lemma  59423 non-null  object \n",
            " 4   text_clean_stem   59423 non-null  object \n",
            " 5   text_clean_emb    59423 non-null  object \n",
            " 6   label_clean       59423 non-null  float64\n",
            "dtypes: float64(1), int64(2), object(4)\n",
            "memory usage: 3.2+ MB\n"
          ]
        }
      ],
      "source": [
        "data_clean.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we are ready our data is cleaned 🙌🙌🙌"
      ],
      "metadata": {
        "id": "aub8RcC72w9B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Shape of train data after cleaning \" , data_clean.shape)\n",
        "print(\"Shape of submission data after cleaning \" , submission_data.shape)\n",
        "\n",
        "print(\"Columns of train data after cleaning \" , data_clean.columns)\n",
        "print(\"Columns of submission data after cleaning \" , submission_data.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVlzJz4a2_ji",
        "outputId": "65a521f8-fd87-4b25-9143-f8ddd81e9f88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of train data after cleaning  (59423, 7)\n",
            "Shape of submission data after cleaning  (59151, 5)\n",
            "Columns of train data after cleaning  Index(['text', 'label', 'length', 'text_clean_lemma', 'text_clean_stem',\n",
            "       'text_clean_emb', 'label_clean'],\n",
            "      dtype='object')\n",
            "Columns of submission data after cleaning  Index(['id', 'text', 'text_clean_lemma', 'text_clean_stem', 'text_clean_emb'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWKZ5JT5ottL"
      },
      "source": [
        "# Split data into train & test\n",
        "----------------------------------------------------------------\n",
        "--------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGKG0BrAmnZi",
        "outputId": "5d2e855e-1b5e-41d9-9474-cb7049244b46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(44567, 7)\n",
            "(14856, 7)\n"
          ]
        }
      ],
      "source": [
        "# Sample data - 25% of data to test set\n",
        "train, test = train_test_split(data_clean, random_state=7777777, test_size=0.25, shuffle=True)\n",
        "\n",
        "\n",
        "X_train_stem = train[\"text_clean_stem\"]\n",
        "Y_train = train[\"label_clean\"]\n",
        "X_test_stem = test[\"text_clean_stem\"]\n",
        "Y_test = test[\"label_clean\"]\n",
        "\n",
        "X_train_lemma = train[\"text_clean_lemma\"]\n",
        "X_test_lemma = test[\"text_clean_lemma\"]\n",
        "\n",
        "X_train_emb = train[\"text_clean_emb\"]\n",
        "X_test_emb = test[\"text_clean_emb\"]\n",
        "\n",
        "\n",
        "\n",
        "print(train.shape)\n",
        "print(test.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0aR_pF9LLMB"
      },
      "source": [
        "**Observation** 💡💡💡💡💡\n",
        "\n",
        "\n",
        "- As you can see, we split a training set (with labels) into 2 sets : train set to train our model and test set to evaluate our model before using it. Test set is unseen data we used after fitting model to know our model performs well or not. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWYNxIw5ottU"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-29T01:16:09.417494Z",
          "iopub.status.busy": "2023-03-29T01:16:09.417043Z",
          "iopub.status.idle": "2023-03-29T01:16:09.424016Z",
          "shell.execute_reply": "2023-03-29T01:16:09.422506Z",
          "shell.execute_reply.started": "2023-03-29T01:16:09.417431Z"
        },
        "id": "5jCFG0ALottU"
      },
      "outputs": [],
      "source": [
        "np.random.seed(1223)\n",
        "\n",
        "#-------------------------------------- Build model ^_^ ----------------------------\n",
        "''' \n",
        "This Function takes model and vectorization method .Then combine them in single pipeline \n",
        "'''\n",
        "\n",
        "def buid_model(model_trail , vectorize_word ):\n",
        "    # combine the preprocessor with the model as a full tunable pipeline\n",
        "    # we gave them a name so we can set their hyperparameters\n",
        "    model = Pipeline(steps=[(\"vectorize\", vectorize_word), (\"model\", model_trail)])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stM6DfzSottU"
      },
      "source": [
        "# Evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-29T01:35:45.148303Z",
          "iopub.status.busy": "2023-03-29T01:35:45.147882Z",
          "iopub.status.idle": "2023-03-29T01:35:45.157155Z",
          "shell.execute_reply": "2023-03-29T01:35:45.155695Z",
          "shell.execute_reply.started": "2023-03-29T01:35:45.148256Z"
        },
        "id": "ruJPbrAUottU"
      },
      "outputs": [],
      "source": [
        "#------------------------------ Evaluation Function ^_^ --------------------------\n",
        "'''\n",
        "This Function takes model and evaluate this model on train and test data through some evaluation matrics such as classification report , auc ,and accuracy.\n",
        "'''\n",
        "def evaluation(model , x_train , x_test , y_test , y_train):\n",
        "    y_pred_train = model.predict(x_train)\n",
        "    y_pred_test = model.predict(x_test)\n",
        "    \n",
        "    class_repo = classification_report(y_test, y_pred_test)\n",
        "    print(\"Classification report\" , class_repo)\n",
        "    \n",
        "    test_acc = accuracy_score(y_test ,y_pred_test)\n",
        "    train_acc = accuracy_score(y_train,y_pred_train)\n",
        "    print(\"Accuracy in train data\" , train_acc)\n",
        "    print(\"Accuracy in test data\" , test_acc)\n",
        "\n",
        "    prob_y_pred = model.predict_proba(x_test)[:,1]\n",
        "    prob_y_pred_train = model.predict_proba(x_train)[:,1]\n",
        "    \n",
        "    #calculate AUC of model\n",
        "    auc = metrics.roc_auc_score(y_test, prob_y_pred)\n",
        "    auc_train = metrics.roc_auc_score(y_train, prob_y_pred_train)\n",
        "    \n",
        "\n",
        "    #print AUC score\n",
        "    print(\"AUC test \" , auc)\n",
        "    print(\"AUC train \" , auc_train)\n",
        "    \n",
        "    return  test_acc , train_acc , auc\n",
        "#------------------------------ Save Model ^_^ --------------------------\n",
        "'''\n",
        "This Function takes model and its name .Then save it in pickle format\n",
        "'''\n",
        "def save_model(model , model_name):\n",
        "    # save model\n",
        "    joblib.dump(model, \"model_{}.pk\".format(model_name))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhmnIiZpottV"
      },
      "source": [
        "# Submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-29T01:16:22.672140Z",
          "iopub.status.busy": "2023-03-29T01:16:22.671711Z",
          "iopub.status.idle": "2023-03-29T01:16:22.679200Z",
          "shell.execute_reply": "2023-03-29T01:16:22.677599Z",
          "shell.execute_reply.started": "2023-03-29T01:16:22.672101Z"
        },
        "id": "4ZpKB3f5ottV"
      },
      "outputs": [],
      "source": [
        "#------------------------------ csv creation Function ^_^ --------------------------\n",
        "'''\n",
        "This Function takes model and submission_data .Then apply model in this data. save the predicted values in csv file \n",
        "'''\n",
        "def submitted_file(model ,  data , model_name):\n",
        "    submission = pd.DataFrame()\n",
        "\n",
        "    submission['id'] = submission_data['id']\n",
        "\n",
        "    submission['label'] = model.predict_proba(data)[:,1]\n",
        "\n",
        "    submission.to_csv('sample_submission_walkthrough_{}.csv'.format(model_name), index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------------------------\n",
        "-------------------------------------\n",
        "-----------------------------------\n",
        "#✔️ Now, we are ready to start our trails \n",
        " \n",
        " - Our approach is that we will try different classifiers and tune these classifiers to find best hyper parameters. Our problem is pretty complex.\n",
        "So, we Chose most powerful models that can handel this problem with high performance. The most powerful models are Randomforest , xgboost , gradient xgboost ,and logistic regression.\n",
        "\n",
        "Let's move on and see the behaviour of each one.\n",
        "---------------------------\n",
        "-------------------------------------\n",
        "--------------------------------"
      ],
      "metadata": {
        "id": "bHZWx-sIF6HE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjBMmUkhottV"
      },
      "source": [
        "# Random Forest Grid Search (1st trail)\n",
        "(Lemmatization )\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- we used randomforest model and applied a lemmatization as a preprocessing method in text. Also, we applied grid search cross validation with 5 kfold to tune this model. we chose the best model that have the highest auc. \n",
        "\n",
        "- In this trail, we covered:\n",
        "\n",
        "  - Lemmatization preprocessing \n",
        "  - A tunable pipeline including word-level vectorizer.\n",
        "  - gridseacrh method with cross validation \n",
        "\n",
        "- we hope that the previous combination improve our performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "OIL4Hh-GONEf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-03-29T01:23:58.875055Z",
          "iopub.status.busy": "2023-03-29T01:23:58.874597Z",
          "iopub.status.idle": "2023-03-29T01:27:09.608043Z",
          "shell.execute_reply": "2023-03-29T01:27:09.606536Z",
          "shell.execute_reply.started": "2023-03-29T01:23:58.875012Z"
        },
        "id": "CHq1kk5vottV",
        "outputId": "59db3ca9-c91c-4ab1-ec26-b7fed414cd0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 7600 candidates, totalling 38000 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   6 tasks      | elapsed:    5.6s\n",
            "[Parallel(n_jobs=-1)]: Done  60 tasks      | elapsed:   31.3s\n",
            "[Parallel(n_jobs=-1)]: Done 150 tasks      | elapsed:  1.3min\n",
            "[Parallel(n_jobs=-1)]: Done 276 tasks      | elapsed:  2.3min\n",
            "[Parallel(n_jobs=-1)]: Done 438 tasks      | elapsed:  3.6min\n",
            "[Parallel(n_jobs=-1)]: Done 636 tasks      | elapsed:  5.1min\n",
            "[Parallel(n_jobs=-1)]: Done 870 tasks      | elapsed:  6.8min\n",
            "[Parallel(n_jobs=-1)]: Done 1140 tasks      | elapsed:  9.3min\n",
            "[Parallel(n_jobs=-1)]: Done 1446 tasks      | elapsed: 12.6min\n",
            "[Parallel(n_jobs=-1)]: Done 1788 tasks      | elapsed: 16.5min\n",
            "[Parallel(n_jobs=-1)]: Done 2166 tasks      | elapsed: 21.5min\n",
            "[Parallel(n_jobs=-1)]: Done 2580 tasks      | elapsed: 27.4min\n",
            "[Parallel(n_jobs=-1)]: Done 3030 tasks      | elapsed: 33.1min\n",
            "[Parallel(n_jobs=-1)]: Done 3516 tasks      | elapsed: 38.1min\n",
            "[Parallel(n_jobs=-1)]: Done 4038 tasks      | elapsed: 43.2min\n",
            "[Parallel(n_jobs=-1)]: Done 4596 tasks      | elapsed: 47.7min\n",
            "[Parallel(n_jobs=-1)]: Done 5190 tasks      | elapsed: 55.0min\n",
            "[Parallel(n_jobs=-1)]: Done 5820 tasks      | elapsed: 63.5min\n",
            "[Parallel(n_jobs=-1)]: Done 6486 tasks      | elapsed: 75.7min\n",
            "[Parallel(n_jobs=-1)]: Done 7188 tasks      | elapsed: 84.9min\n",
            "[Parallel(n_jobs=-1)]: Done 7926 tasks      | elapsed: 92.7min\n",
            "[Parallel(n_jobs=-1)]: Done 8700 tasks      | elapsed: 100.4min\n",
            "[Parallel(n_jobs=-1)]: Done 9510 tasks      | elapsed: 113.8min\n",
            "[Parallel(n_jobs=-1)]: Done 10356 tasks      | elapsed: 134.2min\n",
            "[Parallel(n_jobs=-1)]: Done 11238 tasks      | elapsed: 147.6min\n",
            "[Parallel(n_jobs=-1)]: Done 12156 tasks      | elapsed: 159.9min\n",
            "[Parallel(n_jobs=-1)]: Done 13110 tasks      | elapsed: 193.8min\n",
            "[Parallel(n_jobs=-1)]: Done 14100 tasks      | elapsed: 252.2min\n",
            "[Parallel(n_jobs=-1)]: Done 15126 tasks      | elapsed: 290.1min\n",
            "[Parallel(n_jobs=-1)]: Done 16188 tasks      | elapsed: 305.8min\n",
            "[Parallel(n_jobs=-1)]: Done 17286 tasks      | elapsed: 348.9min\n",
            "[Parallel(n_jobs=-1)]: Done 18420 tasks      | elapsed: 399.7min\n",
            "[Parallel(n_jobs=-1)]: Done 19590 tasks      | elapsed: 419.3min\n",
            "[Parallel(n_jobs=-1)]: Done 20796 tasks      | elapsed: 430.4min\n",
            "[Parallel(n_jobs=-1)]: Done 22038 tasks      | elapsed: 446.6min\n",
            "[Parallel(n_jobs=-1)]: Done 23316 tasks      | elapsed: 457.1min\n",
            "[Parallel(n_jobs=-1)]: Done 24630 tasks      | elapsed: 471.1min\n",
            "[Parallel(n_jobs=-1)]: Done 25980 tasks      | elapsed: 492.4min\n",
            "[Parallel(n_jobs=-1)]: Done 27366 tasks      | elapsed: 504.8min\n",
            "[Parallel(n_jobs=-1)]: Done 28788 tasks      | elapsed: 527.4min\n",
            "[Parallel(n_jobs=-1)]: Done 30246 tasks      | elapsed: 553.3min\n",
            "[Parallel(n_jobs=-1)]: Done 31740 tasks      | elapsed: 580.3min\n",
            "[Parallel(n_jobs=-1)]: Done 33270 tasks      | elapsed: 658.1min\n",
            "[Parallel(n_jobs=-1)]: Done 34836 tasks      | elapsed: 692.6min\n",
            "[Parallel(n_jobs=-1)]: Done 36438 tasks      | elapsed: 748.4min\n",
            "[Parallel(n_jobs=-1)]: Done 38000 out of 38000 | elapsed: 811.1min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best score 0.8286776042192028\n",
            "best params 1st trail (lemma_grid search) {'model__class_weight': 'balanced_subsample', 'model__max_depth': 80, 'model__n_estimators': 170, 'vectorize__max_df': 0.3, 'vectorize__min_df': 5, 'vectorize__ngram_range': (1, 3)}\n"
          ]
        }
      ],
      "source": [
        "Random_forest_model = RandomForestClassifier()\n",
        "Random_forest_model = buid_model(Random_forest_model , TfidfVectorizer())\n",
        "param_grid_1st = {\n",
        "    \"vectorize__ngram_range\": [(1, 2), (1, 3)],\n",
        "    \"vectorize__max_df\": np.arange(0.3, 0.8),\n",
        "    \"vectorize__min_df\": np.arange(5, 100),\n",
        "    'model__n_estimators': [20, 30, 40 , 50],  \n",
        "    'model__max_depth':[10, 20, 30 , 80, 77]  ,\n",
        "    'model__class_weight' : ['balanced', 'balanced_subsample'],\n",
        "    'model__n_estimators' : [20,100,170 ,77]\n",
        "}\n",
        "\n",
        "grid_search_1st = GridSearchCV(Random_forest_model, param_grid_1st, cv=5, verbose=5, n_jobs=-1, scoring='roc_auc')\n",
        "grid_search_1st.fit(X_train_lemma, Y_train)\n",
        "\n",
        "print('best score {}'.format(grid_search_1st.best_score_))\n",
        "print('best params 1st trail (lemma_grid search) {}'.format(grid_search_1st.best_params_))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation 💡💡💡💡💡**\n",
        "- After 13 hours of training 😑😑😑, we got only 83 % auc.\n",
        "- Acually, we weren't lucky enough in choosing our search space. Grid search tried all possible values. Maybe if we choose another search space or model or even changing preprocessing methods, our model will improve.\n",
        "\n",
        "\n",
        "- Don't be too quick to judge. Let's see how the model perforn on unseen data."
      ],
      "metadata": {
        "id": "_Yhg5EXsPnZF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZSfNsOrottW"
      },
      "source": [
        "### Evaluate \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-03-29T01:36:46.644341Z",
          "iopub.status.busy": "2023-03-29T01:36:46.643351Z",
          "iopub.status.idle": "2023-03-29T01:36:47.148529Z",
          "shell.execute_reply": "2023-03-29T01:36:47.147129Z",
          "shell.execute_reply.started": "2023-03-29T01:36:46.644291Z"
        },
        "id": "ArdLMiKiottW",
        "outputId": "0c6a6fbe-a34d-45d4-d0de-57d5d22fed3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification report               precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.84      0.62      0.71      7982\n",
            "         1.0       0.66      0.86      0.75      6960\n",
            "\n",
            "    accuracy                           0.73     14942\n",
            "   macro avg       0.75      0.74      0.73     14942\n",
            "weighted avg       0.76      0.73      0.73     14942\n",
            "\n",
            "Accuracy in train data 0.8635613260161513\n",
            "Accuracy in test data 0.7318297416677821\n",
            "AUC test  0.8298478779120837\n",
            "AUC train  0.9757955219553544\n"
          ]
        }
      ],
      "source": [
        "test_acc , train_acc , auc = evaluation(grid_search_1st , X_train_lemma, X_test_lemma, Y_test , Y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation 💡💡💡💡💡**\n",
        "\n",
        "- As we expected the model isn't good. The model is overfitting as you see the model doesn't perform well  on test data (accuracy test 73%)\n",
        "\n",
        "- The difference between accuracy in train and test is a large \n",
        "- Also, f1score is very low\n",
        "\n",
        "- Random Forest is observed high bias in testing set. This is due to depth is high. we need to trim it. Also, if we change the search space of estimators. Increasing estimators improve the accuracy. Reducing max depth and Increasing estimators improve the performance\n",
        "\n",
        "- Score in Kaggle : 79.512 % auc .we are inspired to train model with best hyperparameters we got on all the entire data to improve the performance.let's see this idea work well or not\n"
      ],
      "metadata": {
        "id": "roBQhaG9S2jJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8Gh7K1f-bDu"
      },
      "source": [
        "### Train model with optimized parameters\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we are inspired to train model with best hyperparameters we got on all the entire data to improve the performance."
      ],
      "metadata": {
        "id": "GgdvH9DJWH8x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cix1Ys3L-pln",
        "outputId": "eb4a6b45-e9c6-4f02-f805-35f955631cfd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Pipeline(steps=[('vectorize',\n",
              "                 TfidfVectorizer(max_df=0.3, min_df=5, ngram_range=(1, 2))),\n",
              "                ('model',\n",
              "                 RandomForestClassifier(class_weight='balanced', max_depth=77,\n",
              "                                        n_estimators=170))])"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_params = grid_search_1st.best_params_\n",
        "Random_forest_model.set_params(**best_params).fit(data_clean['text_clean_lemma'], data_clean['label_clean'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oxry4c82ottW"
      },
      "source": [
        "### Save model\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-29T01:33:24.608849Z",
          "iopub.status.busy": "2023-03-29T01:33:24.608386Z",
          "iopub.status.idle": "2023-03-29T01:33:24.677853Z",
          "shell.execute_reply": "2023-03-29T01:33:24.676490Z",
          "shell.execute_reply.started": "2023-03-29T01:33:24.608806Z"
        },
        "id": "k-gr1r0SottW"
      },
      "outputs": [],
      "source": [
        "save_model(Random_forest_model , 'random_forest_grid_1st_trail_lemma')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-29T01:38:54.886333Z",
          "iopub.status.busy": "2023-03-29T01:38:54.885900Z",
          "iopub.status.idle": "2023-03-29T01:38:55.078366Z",
          "shell.execute_reply": "2023-03-29T01:38:55.076853Z",
          "shell.execute_reply.started": "2023-03-29T01:38:54.886293Z"
        },
        "id": "3f_z59-6ottW"
      },
      "outputs": [],
      "source": [
        "submitted_file(Random_forest_model ,  submission_data['text_clean_lemma'] , 'random_forest_grid_1st_trail_lemma')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUAQrO954osb"
      },
      "source": [
        "**Observation 💡💡💡💡💡**\n",
        "- We noticed a significant improvement in score from  79.512 % to 79.954 %.Increasing training data improves the performance and reduces overfitting.\n",
        "\n",
        "- Score on kaggle :  79.954 %  🥳🥳🥳🥳\n",
        "\n",
        "Let's move on through another trail. But this time, we will change the model and apply the same preprocessing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7NRkPhxp89m"
      },
      "source": [
        "# Xgboost Bayes Search (2nd trail)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- In this trail ,we will use XGboost. The reason why we will make this certain change the most important differences between XG Boost and Random forest that:\n",
        "  -  XGBoost always gives more importance to functional space when reducing the cost of a model while Random Forest tries to give more preferences to hyperparameters to optimize the model. \n",
        "  - A small change in the hyperparameter will affect almost all trees in the forest which can alter the prediction. \n",
        "  - Also, this is not a good approach when we expect test data with so many variations in real-time with a pre-defined mindset of hyperparameters for the whole forest but XG boost hyperparameters are applied to only one tree at the beginning which is expected to adjust itself in an efficient manner when iterations progress. \n",
        "  - Also, the XGBoost needs only a very low number of initial hyperparameters (shrinkage parameter, depth of the tree, number of trees) when compared with the Random forest.\n",
        "\n",
        "[Source](https://medium.com/geekculture/xgboost-versus-random-forest-898e42870f30)\n",
        "\n",
        "Also, we changed the search space of max depth and estimators. As we mentioned before that Reducing max depth and Increasing estimators improve the performance \n",
        "\n",
        "\n",
        "- In this trail, we covered:\n",
        "\n",
        "  - Lemmatization preprocessing \n",
        "  - A tunable pipeline including word-level vectorizer.\n",
        "  - Baysian method validation set (we use this moethod not grid search as the previous trail as Random forests are easier to tune than Boosting algorithms. So, we used baysain instead gridto save our time and get the best hyperparmaters also.) \n",
        "\n",
        "- we hope that the previous combination improve our performance than the previous trail.\n",
        "\n"
      ],
      "metadata": {
        "id": "INz_tplIDjra"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTRSziQj4osb",
        "outputId": "851394c3-3af3-413e-8822-d9895aea3220"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import PredefinedSplit\n",
        "# Further split the original training set to a train and a validation set\n",
        "X_train2, X_val, y_train2, y_val = train_test_split(\n",
        "    X_train_lemma, Y_train, train_size = 0.8, stratify = Y_train, random_state = 2022)\n",
        "\n",
        "split_index = [-1 if x in X_train2.index else 0 for x in X_train_lemma.index]\n",
        "# Use the list to create PredefinedSplit\\n\"\n",
        "pds = PredefinedSplit(test_fold = split_index)\n",
        "print(pds.get_n_splits())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-29T01:23:58.875055Z",
          "iopub.status.busy": "2023-03-29T01:23:58.874597Z",
          "iopub.status.idle": "2023-03-29T01:27:09.608043Z",
          "shell.execute_reply": "2023-03-29T01:27:09.606536Z",
          "shell.execute_reply.started": "2023-03-29T01:23:58.875012Z"
        },
        "id": "29kDKZJ2p89m",
        "outputId": "0a4f87d7-cd35-426e-ea78-416b6ac6728e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    5.5s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    6.2s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   40.8s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    5.8s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   18.5s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    5.4s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    4.5s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   37.9s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  1.1min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    1.2s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    6.8s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    7.3s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   19.2s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   21.0s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    2.0s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   20.6s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   28.4s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   20.6s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   22.8s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   16.9s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   22.4s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   14.8s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   16.5s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   14.9s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   19.3s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   18.4s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   17.9s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   19.2s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   15.9s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   17.3s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   18.6s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   17.8s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   15.5s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   18.1s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   23.1s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   18.9s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   17.1s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   19.0s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   25.4s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   22.6s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   19.6s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   18.0s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   16.7s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   20.3s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   14.7s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   14.6s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   15.0s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   18.7s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   21.2s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   15.0s finished\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "BayesSearchCV(cv=PredefinedSplit(test_fold=array([ 0, -1, ..., -1, -1])),\n",
              "              estimator=Pipeline(steps=[('vectorize', TfidfVectorizer()),\n",
              "                                        ('model',\n",
              "                                         XGBClassifier(base_score=None,\n",
              "                                                       booster=None,\n",
              "                                                       callbacks=None,\n",
              "                                                       colsample_bylevel=None,\n",
              "                                                       colsample_bynode=None,\n",
              "                                                       colsample_bytree=None,\n",
              "                                                       early_stopping_rounds=None,\n",
              "                                                       enable_categorical=False,\n",
              "                                                       eval_metric=None,\n",
              "                                                       feature_types=None,\n",
              "                                                       ga...\n",
              "                             'vectorize__max_df': array([0.3]),\n",
              "                             'vectorize__min_df': array([ 5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21,\n",
              "       22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38,\n",
              "       39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55,\n",
              "       56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72,\n",
              "       73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89,\n",
              "       90, 91, 92, 93, 94, 95, 96, 97, 98, 99])},\n",
              "              verbose=5)"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "xgb_model_1 = XGBClassifier()\n",
        "xgb_model_1st = buid_model(xgb_model_1 ,TfidfVectorizer())\n",
        "param_grid_2nd =  {\n",
        "    \n",
        "    \"vectorize__max_df\": np.arange(0.3, 0.8),\n",
        "    \"vectorize__min_df\": np.arange(5, 100),\n",
        "    'model__n_estimators': [20,77,100,200,500],\n",
        "    'model__max_depth':[5,7,10,15],\n",
        "    'model__subsample':[0.6,0.8,1],\n",
        "    'model__colsample_bytree':[0.5,0.7,1],\n",
        "}\n",
        "\n",
        "bayes_search_xgb_2nd = BayesSearchCV(xgb_model_1st, param_grid_2nd, cv=pds, verbose=5, n_jobs=-1, scoring='roc_auc')\n",
        "bayes_search_xgb_2nd.fit(X_train_lemma, Y_train)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMVHXn814osb",
        "outputId": "46ed1a5b-3d2b-438d-9425-04da2a6389b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best score 0.8564701012414637\n",
            "best params for xgb (2nd trail) OrderedDict([('model__colsample_bytree', 1.0), ('model__max_depth', 15), ('model__n_estimators', 200), ('model__subsample', 1.0), ('vectorize__max_df', 0.3), ('vectorize__min_df', 18)])\n"
          ]
        }
      ],
      "source": [
        "print('best score {}'.format(bayes_search_xgb_2nd.best_score_))\n",
        "print('best params for xgb (2nd trail) {}'.format(bayes_search_xgb_2nd.best_params_))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation 💡💡💡💡💡**\n",
        "\n",
        "- yummmy 😃😃! There is an improvement in our auc from 83 % (using random forest) to 85.6 %.\n",
        "\n",
        "Don't be too quick to judge. High auc isn't enough to say this model is the best. Let's see how the model perform on unseen data."
      ],
      "metadata": {
        "id": "yT_kcyf2JST5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFUC5K7J4osb"
      },
      "source": [
        "### Evaluate \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-03-29T01:36:46.644341Z",
          "iopub.status.busy": "2023-03-29T01:36:46.643351Z",
          "iopub.status.idle": "2023-03-29T01:36:47.148529Z",
          "shell.execute_reply": "2023-03-29T01:36:47.147129Z",
          "shell.execute_reply.started": "2023-03-29T01:36:46.644291Z"
        },
        "outputId": "0c6a6fbe-a34d-45d4-d0de-57d5d22fed3c",
        "id": "lPKBzPKC4osc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification report               precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.77      0.78      7982\n",
            "         1.0       0.74      0.76      0.75      6960\n",
            "\n",
            "    accuracy                           0.76     14942\n",
            "   macro avg       0.76      0.76      0.76     14942\n",
            "weighted avg       0.76      0.76      0.76     14942\n",
            "\n",
            "Accuracy in train data 0.905925132735466\n",
            "Accuracy in test data 0.7622138937223932\n",
            "AUC test  0.8513541603665719\n",
            "AUC train  0.9705164553875475\n"
          ]
        }
      ],
      "source": [
        "test_acc , train_acc , auc = evaluation(bayes_search_xgb_2nd , X_train_lemma, X_test_lemma, Y_test , Y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation 💡💡💡💡💡**\n",
        "\n",
        "- As we expected the model isn't good. The model is overfitting as you see the model doesn't perform well  on test data (accuracy test 76%). the percent of overfitting is the same. XGboost can't handle the overfitting also.\n",
        "\n",
        "- f1score is improved comapred to randomforest.\n",
        "- we excepected that this model will give score higher than random forest.\n",
        "\n",
        "we are inspired to train model with best hyperparameters we got on all the entire data to improve the performance and reduce overfiting.\n",
        "\n"
      ],
      "metadata": {
        "id": "2pbSFRkXOMxm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4VcPum34osc"
      },
      "source": [
        "### Train model with optimized parameters\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IE-ep23g4osc",
        "outputId": "f9447d82-06b7-4f4a-9d09-b6ab7562ecb4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Pipeline(steps=[('vectorize', TfidfVectorizer(max_df=0.3, min_df=18)),\n",
              "                ('model',\n",
              "                 XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
              "                               colsample_bylevel=None, colsample_bynode=None,\n",
              "                               colsample_bytree=1.0, early_stopping_rounds=None,\n",
              "                               enable_categorical=False, eval_metric=None,\n",
              "                               feature_types=None, gamma=None, gpu_id=None,\n",
              "                               grow_policy=None, importance_type=None,\n",
              "                               interaction_constraints=None, learning_rate=None,\n",
              "                               max_bin=None, max_cat_threshold=None,\n",
              "                               max_cat_to_onehot=None, max_delta_step=None,\n",
              "                               max_depth=15, max_leaves=None,\n",
              "                               min_child_weight=None, missing=nan,\n",
              "                               monotone_constraints=None, n_estimators=200,\n",
              "                               n_jobs=None, num_parallel_tree=None,\n",
              "                               predictor=None, random_state=None, ...))])"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_params = bayes_search_xgb_2nd.best_params_\n",
        "xgb_model_1st.set_params(**best_params).fit(data_clean['text_clean_lemma'], data_clean['label_clean'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCpBpun84osc"
      },
      "source": [
        "### Save model\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-29T01:33:24.608849Z",
          "iopub.status.busy": "2023-03-29T01:33:24.608386Z",
          "iopub.status.idle": "2023-03-29T01:33:24.677853Z",
          "shell.execute_reply": "2023-03-29T01:33:24.676490Z",
          "shell.execute_reply.started": "2023-03-29T01:33:24.608806Z"
        },
        "id": "sjtRLhiI4osc"
      },
      "outputs": [],
      "source": [
        "save_model(xgb_model_1st , 'xgb_model_1st_bayes_2nd_trail_lemma')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-29T01:38:54.886333Z",
          "iopub.status.busy": "2023-03-29T01:38:54.885900Z",
          "iopub.status.idle": "2023-03-29T01:38:55.078366Z",
          "shell.execute_reply": "2023-03-29T01:38:55.076853Z",
          "shell.execute_reply.started": "2023-03-29T01:38:54.886293Z"
        },
        "id": "7_TH8y7H4osc"
      },
      "outputs": [],
      "source": [
        "submitted_file(xgb_model_1st ,  submission_data['text_clean_lemma'] , 'xgb_model_1st_bayes_2nd_trail_lemma')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9987eXg4osc"
      },
      "source": [
        "**Observation 💡💡💡💡💡**\n",
        "- We noticed a significant improvement in score from 76 % to 80.102 %. Increasing training data improves the performance and reduces overfitting.\n",
        "\n",
        "- Score on kaggle :  80.102 %  🥳🥳🥳🥳 (Xgboost is the winner till now)\n",
        "\n",
        "Let's move on through another trail. But this time, we will change the model and apply the same preprocessing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUJxK5PB7jzD"
      },
      "source": [
        "# GradientBoosting bayes Search (3rd trail)\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- In this trail ,we will use Gradientboosting classifier. The reason why we will make this certain change that When we aplly tfid vectorization, the number of features is increased. According to this [article](https://towardsdatascience.com/a-brief-introduction-to-xgboost-3eaee2e3e5d6#:~:text=XGBoost%20vs%20Gradient%20Boosting,can%20be%20parallelized%20across%20clusters.). we are inspired to try Gradientboosting\n",
        "\n",
        "\n",
        "- In this trail, we covered:\n",
        "\n",
        "  - Lemmatization preprocessing \n",
        "  - A tunable pipeline including word-level vectorizer.\n",
        "  - Baysian method validation set (we use this moethod not grid search to save our time and get the best hyperparmaters also.) \n",
        "\n",
        "- we hope that the previous combination improve our performance than the previous trail.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Kky5PU7CQKiV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JWYUHUdo4osd",
        "outputId": "7fa7c1f1-2d46-4549-953a-b3196f71d08f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  1.9min finished\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   29.6s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   14.1s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  1.1min finished\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    8.0s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  1.8min finished\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   10.6s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    9.3s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    7.8s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   56.5s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   15.8s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  5.5min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   37.1s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   22.0s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   37.1s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  1.4min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   47.8s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  1.6min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  1.6min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  2.4min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  1.6min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  6.5min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   35.8s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  6.0min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  2.4min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  2.2min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  2.2min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  3.0min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  1.9min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  2.3min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  2.6min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  2.7min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  2.1min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  2.9min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  2.2min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  2.0min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  2.8min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  1.9min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  2.1min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  2.0min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  2.0min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  8.2min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  2.3min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  2.0min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  7.5min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  2.1min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  2.7min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  1.9min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  2.8min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  1.9min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  1.8min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  2.4min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  2.3min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  2.7min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  1.9min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  2.2min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  2.1min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  1.9min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  2.3min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  2.8min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  1.9min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  2.6min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  2.0min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  2.8min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  2.5min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  3.0min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  2.6min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  1.9min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  2.5min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  2.7min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  2.4min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  2.3min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  2.0min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  2.2min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  3.0min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  1.8min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  2.5min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  1.8min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  2.1min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  2.9min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  2.8min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  2.1min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  2.1min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  2.3min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  1.8min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  2.1min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  2.6min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  2.5min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  1.9min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  2.4min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  2.5min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  2.4min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  2.3min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  2.6min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  2.0min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  1.9min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  2.7min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  2.2min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  3.1min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  1.8min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  2.1min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  2.2min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  2.7min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  2.1min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  2.3min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  2.5min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  2.2min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  2.5min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  3.2min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  2.3min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  1.9min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  2.2min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  3.1min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  1.9min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  2.0min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  2.4min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  6.3min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best score 0.8576991921993521\n",
            "best params 3rd trail (lemma_bayes search)  OrderedDict([('model__learning_rate', 0.1), ('model__max_depth', 30), ('model__n_estimators', 200), ('vectorize__max_df', 0.3), ('vectorize__min_df', 5)])\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "GBC_model_1st = GradientBoostingClassifier()\n",
        "GBC_model_1st = buid_model(GBC_model_1st ,TfidfVectorizer())\n",
        "param_grid_3rd = {\n",
        "    \"vectorize__max_df\": np.arange(0.3, 0.8),\n",
        "    \"vectorize__min_df\": np.arange(5, 100),\n",
        "    'model__n_estimators': [20, 30, 40 , 50,200],  \n",
        "    'model__max_depth':[10, 20, 30 , 80, 77]  ,\n",
        "    'model__learning_rate' : [0.01,0.1 , 0.001 , 1]\n",
        "}\n",
        "\n",
        "GBC_model_1st_3rd = BayesSearchCV(GBC_model_1st, param_grid_3rd, cv=pds,n_iter= 117, verbose=5, n_jobs=-1, scoring='roc_auc')\n",
        "GBC_model_1st_3rd.fit(X_train_lemma, Y_train)\n",
        "\n",
        "print('best score {}'.format(GBC_model_1st_3rd.best_score_))\n",
        "print('best params 3rd trail (lemma_bayes search)  {}'.format(GBC_model_1st_3rd.best_params_))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observation 💡💡💡💡💡\n",
        "\n",
        "- There is no significant improvement in our auc from 85.6 % (using Xgboost) to 85.7 %.\n",
        "Don't be too quick to judge. Let's see how the model perform on unseen data."
      ],
      "metadata": {
        "id": "uSr6dfn9bISu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPO7963R4osd"
      },
      "source": [
        "### Evaluate \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-03-29T01:36:46.644341Z",
          "iopub.status.busy": "2023-03-29T01:36:46.643351Z",
          "iopub.status.idle": "2023-03-29T01:36:47.148529Z",
          "shell.execute_reply": "2023-03-29T01:36:47.147129Z",
          "shell.execute_reply.started": "2023-03-29T01:36:46.644291Z"
        },
        "outputId": "0c6a6fbe-a34d-45d4-d0de-57d5d22fed3c",
        "id": "cNPdjBGi4osd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification report               precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.78      0.78      7982\n",
            "         1.0       0.75      0.75      0.75      6960\n",
            "\n",
            "    accuracy                           0.77     14942\n",
            "   macro avg       0.76      0.77      0.76     14942\n",
            "weighted avg       0.77      0.77      0.77     14942\n",
            "\n",
            "Accuracy in train data 0.9872395484763307\n",
            "Accuracy in test data 0.7660286440904832\n",
            "AUC test  0.8495397780782623\n",
            "AUC train  0.9995324106797796\n"
          ]
        }
      ],
      "source": [
        "test_acc , train_acc , auc = evaluation(GBC_model_1st_3rd , X_train_lemma, X_test_lemma, Y_test , Y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation 💡💡💡💡💡**\n",
        "\n",
        "- The model goes worse. The overfitting is increased comapared to xgboost and random forest.The difference between train and test accuracy is increased from 13 % to 20 %. Gradient boosting is the worst one compared to randomfroest and xgboost in handling overfit.\n",
        "\n",
        "- There is no significant improvement in  f1score comapred to Xgboost.\n",
        "\n",
        "- we excepected that this model will give score lower than xgboost.\n",
        "\n",
        "we are inspired to train model with best hyperparameters we got on all the entire data to improve the performance and reduce overfiting."
      ],
      "metadata": {
        "id": "5BCuOAk7blj9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rp84bdLD4osd"
      },
      "source": [
        "### Train model with optimized parameters\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3iJ5qAM4osd",
        "outputId": "a23c85a1-56a3-4659-c8c7-768d3ccbec60"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Pipeline(steps=[('vectorize', TfidfVectorizer(max_df=0.3, min_df=5)),\n",
              "                ('model',\n",
              "                 GradientBoostingClassifier(max_depth=30, n_estimators=200))])"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_params = GBC_model_1st_3rd.best_params_\n",
        "GBC_model_1st.set_params(**best_params).fit(data_clean['text_clean_lemma'], data_clean['label_clean'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eno0EZNU4osd"
      },
      "source": [
        "### Save model\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-29T01:33:24.608849Z",
          "iopub.status.busy": "2023-03-29T01:33:24.608386Z",
          "iopub.status.idle": "2023-03-29T01:33:24.677853Z",
          "shell.execute_reply": "2023-03-29T01:33:24.676490Z",
          "shell.execute_reply.started": "2023-03-29T01:33:24.608806Z"
        },
        "id": "0fCC3hjS4ose"
      },
      "outputs": [],
      "source": [
        "save_model(GBC_model_1st , 'GBC_model_1st_bayes_3rd_trail_lemma')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-29T01:38:54.886333Z",
          "iopub.status.busy": "2023-03-29T01:38:54.885900Z",
          "iopub.status.idle": "2023-03-29T01:38:55.078366Z",
          "shell.execute_reply": "2023-03-29T01:38:55.076853Z",
          "shell.execute_reply.started": "2023-03-29T01:38:54.886293Z"
        },
        "id": "f1T6sQpu4ose"
      },
      "outputs": [],
      "source": [
        "submitted_file(GBC_model_1st ,  submission_data['text_clean_lemma'] , 'GBC_model_1st_bayes_3rd_trail_lemma')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BK5e_Qrj4ose"
      },
      "source": [
        "**Observation 💡💡💡💡💡**\n",
        "\n",
        "- Surperisely , We noticed a significant improvement in score from 80.102 % (using xgboost ) to 81.122% (using gradientboosting). Increasing training data improves the performance and reduces overfitting.\n",
        "\n",
        "- Score on kaggle :  81.122 % 🥳🥳🥳🥳 (although it is achieved high score, I can't garantuee that it will achieve in the private high score.From my point of view ,Xgboost is the best till now)\n",
        "\n",
        "Let's move on through another trail. But this time, we will change the model and apply the same preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9mrEGGt04ose"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pE5oZJb4ose"
      },
      "source": [
        "# logistic Regression (4th trail)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- There is no reason for us to expect that a particular type of model A\n",
        " has to be better in terms of performance from another type of model B\n",
        " in every possible use-case. This extends to what is observed here; while indeed XGBoost models tend to be successful and generally provide competitive results, they are not guaranteed to be better than a logistic regression model in every setting.\n",
        " \n",
        " \n",
        " - In this trail ,we will use Logistic regression classifier. The reason why we will make this certain change that :\n",
        "  \n",
        "  - Xgboost \n",
        "    - When we applied tfid vectorization, the number of features is increased. \n",
        "\n",
        "    - XGBoost performs very well on medium, small, data with subgroups and structured datasets with not too many features [Source](https://neptune.ai/blog/xgboost-everything-you-need-to-know#:~:text=Gradient%20Boosting%20comes%20with%20an,its%20predictions%20easy%20to%20handle.&text=XGBoost%20performs%20very%20well%20on,with%20not%20too%20many%20features.)\n",
        "\n",
        "    - XGBoost does not perform so well on sparse and unstructured data [Source](https://neptune.ai/blog/xgboost-everything-you-need-to-know#:~:text=Gradient%20Boosting%20comes%20with%20an,its%20predictions%20easy%20to%20handle.&text=XGBoost%20performs%20very%20well%20on,with%20not%20too%20many%20features.)\n",
        "\n",
        "  - Gradient boosting (the general family of methods XGBoost is a part of) :\n",
        "      - is great but it is not perfect; for example, usually gradient boosting approaches have poor probability calibration in comparison to logistic regression models. \n",
        "      -  As we noticed, it increases overfiting form the previous trail.\n",
        "\n",
        "    \n",
        "More generally, certain models are inherently more data-demanding so maybe the dataset available is simply not expressive enough.\n",
        "\n",
        "\n",
        "- In this trail, we covered:\n",
        "\n",
        "  - Lemmatization preprocessing \n",
        "  - A tunable pipeline including tfidf Vectorization \n",
        "  - Baysian method validation set (we use this moethod not grid search to save our time and get the best hyperparmaters also.) \n",
        "  - Tune analyzer of tfidf Vectorization (word level and char level vectorization)\n",
        "  - Tune regularization term l1 or l2\n",
        "\n",
        "- we hope that the previous combination improve our performance than the previous trail.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9PDInLBst3B6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s42U2Pbv4ose",
        "outputId": "faf2f82a-8ba4-4d67-8050-dfa1f70a3c92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    1.7s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    1.7s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    1.3s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    1.2s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    1.3s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    1.6s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.9s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.9s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.9s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.5s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.9s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.8s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.5s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.9s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.5s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    1.4s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.5s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    3.1s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    1.1s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    1.1s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.6s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.6s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.5s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    2.0s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.5s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.5s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.6s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.5s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.6s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    2.0s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.8s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.6s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.7s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.5s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.5s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    2.0s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.5s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.6s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.6s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.6s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.5s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    1.9s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.5s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.5s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.5s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.6s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.6s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    1.9s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.5s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.7s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.5s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.6s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.5s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.9s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.5s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.5s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.5s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.5s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.5s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.5s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.5s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.5s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.5s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.5s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.5s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.5s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.5s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.5s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.5s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.5s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.5s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.5s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.5s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.5s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.5s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.5s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.5s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.5s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.5s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.5s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.5s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.4s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.5s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.5s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.5s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.5s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.5s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.5s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.5s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.5s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.5s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.5s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.5s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.5s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.5s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.5s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.5s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.5s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.5s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.5s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best score 0.8751568753985726\n",
            "best params 4th trail (lemma_bayes search)  OrderedDict([('model__C', 1.0), ('model__max_iter', 100), ('model__penalty', 'l2'), ('model__solver', 'liblinear'), ('vectorize__analyzer', 'word'), ('vectorize__max_df', 0.3), ('vectorize__min_df', 6)])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "log_model_1st = LogisticRegression()\n",
        "log_model_1st = buid_model(log_model_1st ,TfidfVectorizer())\n",
        "param_grid_4th = {\n",
        "    'vectorize__analyzer':['char' , 'word'],\n",
        "    \"vectorize__max_df\": np.arange(0.3, 0.8),\n",
        "    \"vectorize__min_df\": np.arange(5, 100),\n",
        "    \"model__solver\" : ['lbfgs','saga','liblinear','sag'],\n",
        "    'model__penalty':['l2' , 'l1']  ,\n",
        "    'model__C' : [0.001, 0.01, 0.1, 1, 10, 100, 1000] ,\n",
        "    'model__max_iter':[100,200,170]\n",
        "    \n",
        "}\n",
        "\n",
        "log_model_1st_4th = BayesSearchCV(log_model_1st, param_grid_4th, cv=pds,n_iter=100, verbose=5, n_jobs=-1, scoring='roc_auc')\n",
        "log_model_1st_4th.fit(X_train_lemma.values, Y_train.values)\n",
        "\n",
        "print('best score {}'.format(log_model_1st_4th.best_score_))\n",
        "print('best params 4th trail (lemma_bayes search)  {}'.format(log_model_1st_4th.best_params_))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nKjWyTF3CrUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observation 💡💡💡💡💡\n",
        "\n",
        "- There is a significant improvement in our auc from 85.7 % (using GradientXgboost) to 87.5 %.It is the highest score till now .\n",
        "\n",
        "\n",
        " Don't be too quick to judge. Let's see how the model perform on unseen data.\n"
      ],
      "metadata": {
        "id": "7S7oUy9TIHaR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVyLwVl94ose"
      },
      "source": [
        "### Train model with optimized parameters\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "FSkEHvcO4ose",
        "outputId": "3a46c817-7baf-4c50-ee14-bb55ad60bf13"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Pipeline(steps=[('vectorize', TfidfVectorizer(max_df=0.3, min_df=6)),\n",
              "                ('model', LogisticRegression(solver='liblinear'))])"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_params = log_model_1st_4th.best_params_\n",
        "log_model_1st.set_params(**best_params).fit(data_clean['text_clean_lemma'], data_clean['label_clean'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVeClncO4osf",
        "outputId": "08f56a46-74e1-42f3-f326-8712221cbbbb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification report               precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.80      0.79      0.80      7982\n",
            "         1.0       0.76      0.78      0.77      6960\n",
            "\n",
            "    accuracy                           0.78     14942\n",
            "   macro avg       0.78      0.78      0.78     14942\n",
            "weighted avg       0.78      0.78      0.78     14942\n",
            "\n",
            "Accuracy in train data 0.8415205461116316\n",
            "Accuracy in test data 0.7842323651452282\n",
            "AUC test  0.8687917786283506\n",
            "AUC train  0.922420644825672\n"
          ]
        }
      ],
      "source": [
        "test_acc , train_acc , auc = evaluation(log_model_1st_4th , X_train_lemma, X_test_lemma, Y_test , Y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation 💡💡💡💡💡**\n",
        "\n",
        "- The model reduced the overfit compared to other models.The difference between train and test accuracy is reduced from 13 % to 6 %.\n",
        "\n",
        "- There is an improvement in f1score comapred to other models.\n",
        "\n",
        "- we excepected that this model will give highest score.\n",
        "- Let's train model on all entire data to improve the performance\n"
      ],
      "metadata": {
        "id": "LHap5VTiDW-Z"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9u4n2xmv4osf"
      },
      "source": [
        "### Save model\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-29T01:33:24.608849Z",
          "iopub.status.busy": "2023-03-29T01:33:24.608386Z",
          "iopub.status.idle": "2023-03-29T01:33:24.677853Z",
          "shell.execute_reply": "2023-03-29T01:33:24.676490Z",
          "shell.execute_reply.started": "2023-03-29T01:33:24.608806Z"
        },
        "id": "d_E70Tjj4osf"
      },
      "outputs": [],
      "source": [
        "save_model(log_model_1st , 'log_model_1st_bayes_4th_trail_lemma')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-29T01:38:54.886333Z",
          "iopub.status.busy": "2023-03-29T01:38:54.885900Z",
          "iopub.status.idle": "2023-03-29T01:38:55.078366Z",
          "shell.execute_reply": "2023-03-29T01:38:55.076853Z",
          "shell.execute_reply.started": "2023-03-29T01:38:54.886293Z"
        },
        "id": "3oxECCRG4osf"
      },
      "outputs": [],
      "source": [
        "submitted_file(log_model_1st ,  submission_data['text_clean_lemma'] , 'log_model_1st_bayes_4th_trail_lemma')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dB_ZaOE_4osf"
      },
      "source": [
        " **Observation 💡💡💡💡💡**\n",
        "\n",
        "Surperisely , We noticed a significant improvement in score from 81.122% (using gradientboosting) to 83.4 %  (using  Logistic regression ).\n",
        "\n",
        "Score on kaggle : 83.4 % 🥳🥳🥳🥳 (The Logistic regression is the winner model)\n",
        "\n",
        "Let's move on through another trail. But this time, we will apply different preprocessing on the same model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNER8p-A4osg"
      },
      "source": [
        "# logistic Regression (5th trail)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this trail, we try to know the suitable preprocessing method that improves the performance.\n",
        "\n",
        "- In this trail, we covered:\n",
        "\n",
        "  - Stemming preprocessing\n",
        "  - A tunable pipeline including tfidf Vectorization\n",
        "  - Grid search method validation set \n",
        "  - Tune analyzer of tfidf Vectorization (word level and char level vectorization)\n",
        "  - Tune regularization term l1 or l2 to reduce overfitting\n",
        "\n",
        "we hope that the previous combination improve our performance than the previous trail."
      ],
      "metadata": {
        "id": "mosHSIhbFVqY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35KIFAZa4osg",
        "outputId": "411f002e-6a11-4271-92a9-076eff31c1c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 23940 candidates, totalling 23940 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   6 tasks      | elapsed:    3.0s\n",
            "[Parallel(n_jobs=-1)]: Done  60 tasks      | elapsed:   12.0s\n",
            "[Parallel(n_jobs=-1)]: Done 150 tasks      | elapsed:   23.5s\n",
            "[Parallel(n_jobs=-1)]: Done 276 tasks      | elapsed:   43.7s\n",
            "[Parallel(n_jobs=-1)]: Done 438 tasks      | elapsed:  1.2min\n",
            "[Parallel(n_jobs=-1)]: Done 636 tasks      | elapsed:  1.6min\n",
            "[Parallel(n_jobs=-1)]: Done 870 tasks      | elapsed:  2.1min\n",
            "[Parallel(n_jobs=-1)]: Done 1140 tasks      | elapsed:  2.7min\n",
            "[Parallel(n_jobs=-1)]: Done 1446 tasks      | elapsed:  3.6min\n",
            "[Parallel(n_jobs=-1)]: Done 1788 tasks      | elapsed:  4.4min\n",
            "[Parallel(n_jobs=-1)]: Done 2166 tasks      | elapsed:  5.3min\n",
            "[Parallel(n_jobs=-1)]: Done 2580 tasks      | elapsed:  6.4min\n",
            "[Parallel(n_jobs=-1)]: Done 3030 tasks      | elapsed:  7.4min\n",
            "[Parallel(n_jobs=-1)]: Done 3516 tasks      | elapsed:  8.7min\n",
            "[Parallel(n_jobs=-1)]: Done 4038 tasks      | elapsed: 10.0min\n",
            "[Parallel(n_jobs=-1)]: Done 4596 tasks      | elapsed: 11.4min\n",
            "[Parallel(n_jobs=-1)]: Done 5190 tasks      | elapsed: 12.9min\n",
            "[Parallel(n_jobs=-1)]: Done 5820 tasks      | elapsed: 14.4min\n",
            "[Parallel(n_jobs=-1)]: Done 6486 tasks      | elapsed: 16.0min\n",
            "[Parallel(n_jobs=-1)]: Done 7188 tasks      | elapsed: 17.8min\n",
            "[Parallel(n_jobs=-1)]: Done 7926 tasks      | elapsed: 19.7min\n",
            "[Parallel(n_jobs=-1)]: Done 8700 tasks      | elapsed: 21.6min\n",
            "[Parallel(n_jobs=-1)]: Done 9510 tasks      | elapsed: 23.8min\n",
            "[Parallel(n_jobs=-1)]: Done 10356 tasks      | elapsed: 26.0min\n",
            "[Parallel(n_jobs=-1)]: Done 11238 tasks      | elapsed: 29.0min\n",
            "[Parallel(n_jobs=-1)]: Done 12156 tasks      | elapsed: 31.3min\n",
            "[Parallel(n_jobs=-1)]: Done 13110 tasks      | elapsed: 34.7min\n",
            "[Parallel(n_jobs=-1)]: Done 14100 tasks      | elapsed: 38.1min\n",
            "[Parallel(n_jobs=-1)]: Done 15126 tasks      | elapsed: 45.7min\n",
            "[Parallel(n_jobs=-1)]: Done 16188 tasks      | elapsed: 55.5min\n",
            "[Parallel(n_jobs=-1)]: Done 17286 tasks      | elapsed: 64.9min\n",
            "[Parallel(n_jobs=-1)]: Done 18420 tasks      | elapsed: 73.3min\n",
            "[Parallel(n_jobs=-1)]: Done 19590 tasks      | elapsed: 86.7min\n",
            "[Parallel(n_jobs=-1)]: Done 20796 tasks      | elapsed: 98.7min\n",
            "[Parallel(n_jobs=-1)]: Done 22038 tasks      | elapsed: 108.6min\n",
            "[Parallel(n_jobs=-1)]: Done 23316 tasks      | elapsed: 123.0min\n",
            "[Parallel(n_jobs=-1)]: Done 23940 out of 23940 | elapsed: 133.6min finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best score 0.8698143492570124\n",
            "best params 5th trail (stem_bayes search)  {'model__C': 1, 'model__max_iter': 100, 'model__penalty': 'l2', 'model__solver': 'liblinear', 'vectorize__analyzer': 'word', 'vectorize__max_df': 0.3, 'vectorize__min_df': 5}\n"
          ]
        }
      ],
      "source": [
        "log_model_2nd = LogisticRegression()\n",
        "log_model_2nd = buid_model(log_model_2nd ,TfidfVectorizer())\n",
        "param_grid_5th = {\n",
        "    'vectorize__analyzer':['char' , 'word'],\n",
        "    \"vectorize__max_df\": np.arange(0.3, 0.8),\n",
        "    \"vectorize__min_df\": np.arange(5, 100),\n",
        "    \"model__solver\" : ['lbfgs','saga','liblinear'],\n",
        "    'model__penalty':['l2' , 'l1']  ,\n",
        "    'model__C' : [0.001, 0.01, 0.1, 1, 10, 100, 1000] ,\n",
        "    'model__max_iter':[100,200,170]\n",
        "    \n",
        "}\n",
        "\n",
        "log_model_2nd_5th = GridSearchCV(log_model_2nd, param_grid_5th, cv=pds, verbose=5, n_jobs=-1, scoring='roc_auc')\n",
        "log_model_2nd_5th.fit(X_train_stem, Y_train)\n",
        "\n",
        "print('best score {}'.format(log_model_2nd_5th.best_score_))\n",
        "print('best params 5th trail (stem_bayes search)  {}'.format(log_model_2nd_5th.best_params_))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation 💡💡💡💡💡**\n",
        "\n",
        "- We noticed that the auc score is decreased with no significant difference. It is about 0.5 % decreased than the previous trail.\n",
        "Don't be too quick to judge. Let's see how the model perform on unseen data.\n",
        "\n"
      ],
      "metadata": {
        "id": "ohaAFGY5GHdJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIu1ox3T4osg"
      },
      "source": [
        "### Train model with optimized parameters\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "ep4IlW7p4osg",
        "outputId": "401e74fe-8980-48b4-8358-8338e2805947"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Pipeline(steps=[('vectorize', TfidfVectorizer(max_df=0.3, min_df=5)),\n",
              "                ('model', LogisticRegression(C=1, solver='liblinear'))])"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_params = log_model_2nd_5th.best_params_\n",
        "log_model_2nd.set_params(**best_params).fit(data_clean['text_clean_stem'], data_clean['label_clean'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "RA2x0wSs4osg",
        "outputId": "df99507c-5740-422a-89aa-7d87334c4825"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification report               precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.80      0.79      0.80      7982\n",
            "         1.0       0.76      0.77      0.77      6960\n",
            "\n",
            "    accuracy                           0.78     14942\n",
            "   macro avg       0.78      0.78      0.78     14942\n",
            "weighted avg       0.78      0.78      0.78     14942\n",
            "\n",
            "Accuracy in train data 0.8345156828626243\n",
            "Accuracy in test data 0.7826930799089814\n",
            "AUC test  0.8641710551326691\n",
            "AUC train  0.9158255206628471\n"
          ]
        }
      ],
      "source": [
        "test_acc , train_acc , auc = evaluation(log_model_2nd_5th , X_train_stem, X_test_stem, Y_test , Y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation 💡💡💡💡💡**\n",
        "\n",
        "- The model reduced the overfit compared to other models.The improvemnet is not signficant than the previous trail\n",
        "\n",
        "- There is no improvement in f1score compared to the previous trail\n",
        "\n",
        "- we excepected that this model will give score same as the previous trail.\n",
        "- Let's train model on all entire data to improve the performance\n"
      ],
      "metadata": {
        "id": "sbMyAUwVG1Po"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ANgUHqA4osg"
      },
      "source": [
        "### Save model\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-29T01:33:24.608849Z",
          "iopub.status.busy": "2023-03-29T01:33:24.608386Z",
          "iopub.status.idle": "2023-03-29T01:33:24.677853Z",
          "shell.execute_reply": "2023-03-29T01:33:24.676490Z",
          "shell.execute_reply.started": "2023-03-29T01:33:24.608806Z"
        },
        "id": "vAktjall4osg"
      },
      "outputs": [],
      "source": [
        "save_model(log_model_2nd , 'log_model_2nd_grid_5th_trail_stem')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-29T01:38:54.886333Z",
          "iopub.status.busy": "2023-03-29T01:38:54.885900Z",
          "iopub.status.idle": "2023-03-29T01:38:55.078366Z",
          "shell.execute_reply": "2023-03-29T01:38:55.076853Z",
          "shell.execute_reply.started": "2023-03-29T01:38:54.886293Z"
        },
        "id": "g6LkrKYF4osh"
      },
      "outputs": [],
      "source": [
        "submitted_file(log_model_2nd ,  submission_data['text_clean_stem'] , 'log_model_2nd_grid_5th_trail_stem')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRQ00W_-4osh"
      },
      "source": [
        "Observation 💡💡💡💡💡\n",
        "\n",
        "- Surperisely , We noticed an improvement in score from 83.4% (using Logistic with lemmatization ) to 83.4787 %(using Logistic with stemming ). \n",
        "\n",
        "Score on kaggle : 83.4787 %🥳🥳🥳🥳 (The best model is Logistic and the best preprocessing method is stemming)\n",
        "\n",
        "Let's move on through another trail. But this time, we will apply the word level on the same model and apply the same preprocessing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrjuX09t4osh"
      },
      "source": [
        "# logistic Regression (6th trail)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we apply the word levwl as it is required in the statement. But before we try it , we are expected that it won't achieve a good score "
      ],
      "metadata": {
        "id": "unHrmmq8IKaP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8YOJzfX4osh",
        "outputId": "edb8eedf-e343-4ea3-9c82-e5ef116e4055"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    2.4s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    1.9s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    1.8s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    1.9s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    1.9s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    1.9s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.8s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.8s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.8s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.9s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.8s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.8s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.8s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.8s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.8s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.8s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.9s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.8s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.8s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.8s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.8s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.8s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.9s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.8s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.8s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.8s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.9s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.9s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.8s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.9s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.8s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.8s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.9s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.9s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.8s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.9s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.8s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.8s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.9s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.8s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.8s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.8s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.9s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.8s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.9s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.9s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.8s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.9s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.9s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.9s finished\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best score 0.5359765410942441\n",
            "best params 6th trail (stem_bayes search)  OrderedDict([('model__C', 0.01), ('model__max_iter', 100), ('model__penalty', 'l2'), ('model__solver', 'liblinear'), ('vectorize__analyzer', 'char'), ('vectorize__max_df', 0.3), ('vectorize__min_df', 90)])\n"
          ]
        }
      ],
      "source": [
        "log_model_3rd = LogisticRegression()\n",
        "log_model_3rd = buid_model(log_model_3rd ,TfidfVectorizer())\n",
        "param_grid_6th = {\n",
        "    'vectorize__analyzer':['char'],\n",
        "    \"vectorize__max_df\": np.arange(0.3, 0.8),\n",
        "    \"vectorize__min_df\": np.arange(5, 100),\n",
        "    \"model__solver\" : ['lbfgs','saga','liblinear'],\n",
        "    'model__penalty':['l2' ]  ,\n",
        "    'model__C' : [0.001, 0.01, 0.1, 1, 10, 100, 1000] ,\n",
        "    'model__max_iter':[100,200,170]\n",
        "    \n",
        "}\n",
        "\n",
        "log_model_3rd_6th = BayesSearchCV(log_model_3rd, param_grid_6th, cv=pds, verbose=5, n_jobs=-1, scoring='roc_auc')\n",
        "log_model_3rd_6th.fit(X_train_stem, Y_train)\n",
        "\n",
        "print('best score {}'.format(log_model_3rd_6th.best_score_))\n",
        "print('best params 6th trail (stem_bayes search)  {}'.format(log_model_3rd_6th.best_params_))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation 💡💡💡💡💡**\n",
        "\n",
        "As we expected, the model goes worst. The score is very low"
      ],
      "metadata": {
        "id": "g-x-JEKZIhVZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcmQz6-U4osh"
      },
      "source": [
        "### Train model with optimized parameters\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "7yZt3ghM4osi",
        "outputId": "5bcd1d9b-bbc9-49b3-b383-d506e25dfde9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Pipeline(steps=[('vectorize',\n",
              "                 TfidfVectorizer(analyzer='char', max_df=0.3, min_df=90)),\n",
              "                ('model', LogisticRegression(C=0.01, solver='liblinear'))])"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_params = log_model_3rd_6th.best_params_\n",
        "log_model_3rd.set_params(**best_params).fit(data_clean['text_clean_stem'], data_clean['label_clean'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "RkP9uJia4osi",
        "outputId": "9ed919db-5c32-406f-d261-287d0d7ef1b0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification report               precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.53      1.00      0.70      7982\n",
            "         1.0       0.00      0.00      0.00      6960\n",
            "\n",
            "    accuracy                           0.53     14942\n",
            "   macro avg       0.27      0.50      0.35     14942\n",
            "weighted avg       0.29      0.53      0.37     14942\n",
            "\n",
            "Accuracy in train data 0.5396421719537768\n",
            "Accuracy in test data 0.5341989024227011\n",
            "AUC test  0.5294883765051827\n",
            "AUC train  0.5341256757717243\n"
          ]
        }
      ],
      "source": [
        "test_acc , train_acc , auc = evaluation(log_model_3rd_6th , X_train_stem, X_test_stem, Y_test , Y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation 💡💡💡💡💡**\n",
        "\n",
        "- The model is underfit.It can't performs well on the train data. Accuracy is very low.\n",
        "\n",
        "- The model is baised to class 1.\n",
        "- Char level preprocessing doesn't suitable with our probem.\n"
      ],
      "metadata": {
        "id": "XSteerPSIwFh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ue0hBC-L4osi"
      },
      "source": [
        "### Save model\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-29T01:33:24.608849Z",
          "iopub.status.busy": "2023-03-29T01:33:24.608386Z",
          "iopub.status.idle": "2023-03-29T01:33:24.677853Z",
          "shell.execute_reply": "2023-03-29T01:33:24.676490Z",
          "shell.execute_reply.started": "2023-03-29T01:33:24.608806Z"
        },
        "id": "wZeBgd0V4osi"
      },
      "outputs": [],
      "source": [
        "save_model(log_model_3rd , 'log_model_3rd_bayes_6th_trail_stem')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-29T01:38:54.886333Z",
          "iopub.status.busy": "2023-03-29T01:38:54.885900Z",
          "iopub.status.idle": "2023-03-29T01:38:55.078366Z",
          "shell.execute_reply": "2023-03-29T01:38:55.076853Z",
          "shell.execute_reply.started": "2023-03-29T01:38:54.886293Z"
        },
        "id": "JOdWvTD-4osi"
      },
      "outputs": [],
      "source": [
        "submitted_file(log_model_3rd ,  submission_data['text_clean_stem'] , 'log_model_3rd_bayes_6th_trail_stem')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STV-LhaF4osi"
      },
      "source": [
        "# Conclusion  \n",
        "\n",
        "-  Logistic Regression with stemming and word level is the best model. It is achieved the highest Score \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "_ixh-cfmJ1yp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlFJRp5SbNb2"
      },
      "source": [
        "#  🌈 What is the difference between Character n-gram and Word n-gram? Which one tends to suffer more from the OOV issue?\n",
        "\n",
        "\n",
        "\n",
        "Character n-gram refers to a sequence of characters of a given length, while Word n-gram refers to a sequence of words of a given length.\n",
        "\n",
        "Character n-gram tends to suffer more from the OOV (Out of Vocabulary) issue, as it considers individual characters rather than words. OOV occurs when a word that is not in the training data is encountered in the testing or production data. Character n-grams may not capture the semantics of a language and may result in the creation of garbage n-grams that may later negatively affect the performance of the algorithm.\n",
        "\n",
        "Word n-grams, on the other hand, are more representative of the language, and since they consider words as units, they capture the semantics of the language. However, they suffer from the OOV issue, especially when it comes to infrequent and rare words.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 🌈What is the difference between stop word removal and stemming? Are these techniques language-dependent?\n",
        "\n",
        "\n",
        "Stop word removal and stemming are two different techniques used in natural language processing.\n",
        "\n",
        "Stop word removal is the process of eliminating common words that do not carry much meaning in a sentence, such as \"the,\" \"and,\" and \"a.\" These words are removed from a text to improve efficiency and processing speed.\n",
        "\n",
        "Stemming, on the other hand, is the process of reducing words to their root or base form, which can help with clustering and grouping similar words together. For example, \"running,\" \"runner,\" and \"run\" would all be reduced to \"run.\"\n",
        "\n",
        "Both stop word removal and stemming are language-dependent, as different languages have different stop words and stemming rules. For example, in English, the words \"a,\" \"an,\" and \"the\" are common stop words, whereas in French, the words \"et,\" \"le,\" and \"de\" are common stop words. Similarly, stemming rules for English language may not apply to other languages, such as Arabic or Chinese, as these languages have different grammar rules and word structures.\n",
        "\n",
        "\n",
        "# 🌈Is tokenization techniques language dependent? Why?\n",
        "\n",
        "Tokenization techniques are language dependent because the rules for dividing text into individual tokens or units vary between different languages. For example, in English, spaces and punctuation marks usually indicate the boundaries between words, but in Chinese, words themselves do not have spaces, and the division of text into individual units is done based on characters, phrases or compound nouns. Similarly, different programming languages have their own rules for tokenizing code, such as using whitespace or specific characters such as semicolons. Therefore, the tokenization technique suitable for one language might not be appropriate for another language.\n",
        "\n",
        "\n",
        "\n",
        "# 🌈What is the difference between count vectorizer and tf-idf vectorizer? Would it be feasible to use all possible n-grams? If not, how should you select them?\n",
        "\n",
        "Count vectorizer and tf-idf vectorizer are both methods of converting textual data into numerical vectors that can be used for machine learning models. Count vectorizer simply counts the frequency of each word in a text and creates a vector with these counts as features. On the other hand, TF-IDF vectorizer assigns a weight to each word based on its frequency in the text and its overall frequency in the corpus of texts. This weight indicates how important a word is to the document and corpus.\n",
        "\n",
        "It is not feasible to use all possible n-grams because it would create a huge sparsity problem in the data. This is because, as the length of the n-grams increases, the number of unique n-grams in the corpus increases exponentially. This would lead to sparse data with many zeros, making it difficult to perform machine learning algorithms. Therefore, it is essential to select the right n-grams for the given dataset, depending on factors like the size of the dataset, the nature of the text, the problem statement, and the resources available for training. One common approach to this is to experiment with different n-gram ranges and select the range that yields the best performance on the evaluation metric of interest.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "gd9wYusIJ45P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **✔️ Problem Formulation**\n",
        "\n",
        "# ✔️  Define the problem\n",
        "\n",
        "  - The important part before starting to do any thing, we understand the problem very well to be able to be soundness about features you need and features have effect on your prediction.\n",
        "  \n",
        "  - Our goal of this problem is to predict if a specific reddit post is fake news or not, by looking at its title. As we knew that False information on the Internet has caused many social problems due to the raise of social network and its role in different. So, it is important to design model helps us in classifying our news. \n",
        "  \n",
        "- Our output is in categories from 0 to 1. This is a binary classification task. Given a data sample (contains various forms of words), we are going to predict the probability (0-1, float).\n",
        "  \n",
        "\n",
        "\n",
        "# ✔️  What is the input?\n",
        "The input data we're dealing with is in a text format. The title of the news is represented by a number of words in each record. It's a labelled data set with labels of 1 or 0, indicating if the news is fake or not.\n",
        "\n",
        "Note : the labels are not only 0 or 1 but also there is 2 , but we will handle that in the preprocessing section\n",
        "\n",
        "# ✔️  What is the output?\n",
        "The output is the probability for the prediction of whether or not the given news title is fake.\n",
        "\n",
        "# ✔️  What data mining function is required?\n",
        "\n",
        "Note: Because we are predicting chance of whether or not the given news title is fake, AUROC (or ROCAUC) is utilised as the evaluation metric on the predictions.\n",
        "\n",
        "1- Importing the Libraries & Loading the Data : The data is imported\n",
        "\n",
        "2- Data exploration : using seaborn and matplotlib libraries, We could eplore data check nulls columns , duplications ,visualize distribution of data etc.\n",
        "\n",
        "3- Data Preprocessing: In this step we need to \n",
        "  - Remove Duplicated rows\n",
        "  - Removing column with unique value, like [id] Because it will not be useful at all for samples\n",
        "  - Drop the record with '2' value in the 'label' column\n",
        "  - clean the data \n",
        "      - lower case : convert all Chars to lower case\n",
        "      - Remove_Punctuation.\n",
        "      - Remove_Whitespaces\n",
        "      - Remove_Stepwords\n",
        "      - Remove_URLs\n",
        "      - Remove_Digits\n",
        "      - Special Chars removal.\n",
        "      - we try to apply stemming and lemmatization and see the result in each case\n",
        "  - Vectorize text to make it ready for any type of model classification ,by using 'piplines' from scikit learn library to perform steps in sequence . And TF-IDF Vectorizer That covers a character-level vectorizer and word-level vectorizer, to Convert a collection of raw documents to a matrix of TF-IDF features. But we will do this step in two different ways.\n",
        "\n",
        "\n",
        "\n",
        "4- Models: The models used are classification models. We used 3 different classification models which the are:\n",
        "\n",
        "- Random forest 'one version'\n",
        "\n",
        "- XGBoost 'one version'\n",
        "\n",
        "- Gradient Xgboost 'one version'\n",
        "\n",
        "- Logistic Regression '3 different versions'\n",
        "\n",
        "\n",
        "\n",
        "# ✔️  What could be the challenges?\n",
        "The callenges were that\n",
        "\n",
        "-  some algorithms can get distracted from predicting the correct values Because of the large amount of data that needs a good cleaning\n",
        "\n",
        "- the implementation of effective preprocessing processes\n",
        "- the selection of appropriate features to increase model performance, are considered to be challenging.\n",
        "- How to choosing the optimal hyperparameters\n",
        "- How to improve the performance of the models\n",
        "\n",
        "# ✔️  What is the impact?\n",
        "The benefit of not spreading rumors through social networking sites will reduce the damage resulting from misleading the truth and will make social networking sites trusted by people.\n",
        "\n",
        "# ✔️ What is an ideal solution?\n",
        "- An ideal solution will effectively address these problems by optimising hyperparameters to make the most correct estimates possible.\n",
        "\n",
        "- In general, The ideal solution is to develope a machine learning model that properly predicts fake news based on the given features while also being able to handle missing or noisy data would be an ideal option. The model should also be simple to grasp so that firms can identify the most essential criteria for forecasting fake news. Furthermore, the model should be updated on a regular basis with new data to ensure its correctness over time.\n",
        "\n",
        "- The ideal solution of this problem will be a model that has a highest auc score as we want the model to be accurate in classifing news fake or not.The best solution we found through our trials is Logistic Regression with GRid search (using stemming method ) achieved high score in kaggle then  Logistic Regression  with baysian search (using lemmatization method )\n",
        "\n",
        "# ✔️  What is the experimental protocol used and how was it carried out? & What preprocessing steps are used?\n",
        "\n",
        "The experimental protocol that has used is :\n",
        "\n",
        "Different methodologies for Data Preprocessing with different models are used in data preparation measures.\n",
        "\n",
        "\n",
        "\n",
        "The preprocessing steps are used:\n",
        "  - Remove Duplicated rows\n",
        "  - Removing column with unique value, like [id] Because it will not be useful at all for samples\n",
        "  - Drop the record with '2' value in the 'label' column\n",
        "  - clean the data \n",
        "      - lower case : convert all Chars to lower case\n",
        "      - Remove_Punctuation.\n",
        "      - Remove_Whitespaces\n",
        "      - Remove_Stepwords\n",
        "      - Remove_URLs\n",
        "      - Remove_Digits\n",
        "      - Special Chars removal.\n",
        "      - we try to apply stemming and lemmatization and see the result in each case\n",
        "  - Vectorize text to make it ready for any type of model classification ,by using 'piplines' from scikit learn library to perform steps in sequence . And TF-IDF Vectorizer That covers a character-level vectorizer and word-level vectorizer, to Convert a collection of raw documents to a matrix of TF-IDF features. But we will do this step in two different ways.\n",
        "\n"
      ],
      "metadata": {
        "id": "NpkPoEhn5mgX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6d43V3cJCND_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "PQZP5imCottH",
        "ljvxclcqottK",
        "Rrwh6lhYottP",
        "cL0dr7eLottM",
        "4fLbGvRpottM",
        "IMY4CP8vottO",
        "7O3sE7YGottN",
        "Qy85zL8sEX-k",
        "jpkGAnM0ottO",
        "K72V3n_RottQ",
        "78WzMrkEottQ",
        "Uveqqj_5ottS",
        "FWKZ5JT5ottL",
        "hWYNxIw5ottU",
        "stM6DfzSottU",
        "qhmnIiZpottV",
        "jjBMmUkhottV",
        "Q7NRkPhxp89m",
        "eUJxK5PB7jzD",
        "-pE5oZJb4ose",
        "oNER8p-A4osg",
        "GrjuX09t4osh",
        "STV-LhaF4osi"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}